{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aFh4SSnScgyT"
   },
   "source": [
    "# Model Build - Smoothed Series\n",
    "Building and testing models notebook for Google Colab\n",
    "\n",
    "Taking what we learnt from the last notebook, we will now model on a smoothed series instead of the raw data.\n",
    "\n",
    "The rational is that the data is very noisy. It has outliers that would be hard to predict and they also disrupt the patterns that are typically found in the series. If we denoise the series by smoothing it, the pattern will be more apparent and thus our model can tune its parameters better.\n",
    "\n",
    "A few filters were experimented with (e.g. Median filter). I settled on a Gaussian filter which averages out the values in the series in accordance to a normal distribution. \n",
    "\n",
    "\n",
    "Ultimately we wont be predicting the actual temperature anymore but rather a smoothed rendition of the temperature. So in the grid search I added an extra component that calcualtes MAE from scratch by comparing the preds to the actual original temperature values.\n",
    "\n",
    "\n",
    "Be sure to switch to GPU in the run time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2220,
     "status": "ok",
     "timestamp": 1621887955451,
     "user": {
      "displayName": "1990JAF",
      "photoUrl": "",
      "userId": "13601147066600634770"
     },
     "user_tz": -60
    },
    "id": "41wVApIlcCcg",
    "outputId": "969ec8b2-53b8-49b9-fd06-44e85b9bb8d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import InputLayer, LSTM, GRU, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n",
    "\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "\n",
    "print(tf.config.list_physical_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 337,
     "status": "ok",
     "timestamp": 1621887955785,
     "user": {
      "displayName": "1990JAF",
      "photoUrl": "",
      "userId": "13601147066600634770"
     },
     "user_tz": -60
    },
    "id": "HteX1ohWdC35",
    "outputId": "b89cc505-2437-47bb-90d0-869082a5754e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_temp_model.h5\t      results_table_temp.csv\n",
      "results_table_1621536110.csv  results_table_temp_smooth.csv\n",
      "results_table_1621583745.csv  temp_model.h5\n",
      "results_table_1621688232.csv  weather_data.csv\n",
      "results_table_1621765059.csv\n"
     ]
    }
   ],
   "source": [
    "#my file path to data on Gdrive\n",
    "! ls drive/MyDrive/0_neural_net_weather_forecasts_on_cloud/Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D978SAl6Easn"
   },
   "outputs": [],
   "source": [
    "os.chdir('drive/MyDrive/0_neural_net_weather_forecasts_on_cloud/Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HlFbPxxnc8nm"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('weather_data.csv')\n",
    "\n",
    "#get temp and time\n",
    "df['datetime'] = pd.to_datetime(df['datetime'], format='%d/%m/%Y')\n",
    "df = df.set_index('datetime')\n",
    "\n",
    "temp = df['temp']\n",
    "\n",
    "#split data (Save a week for testing. Train and Validation made in class)\n",
    "temp_train = temp.iloc[:-7]\n",
    "temp_test = temp.iloc[-7:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "618XE3dIA99N"
   },
   "source": [
    "##Define the model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0iLf4dZsdRLY"
   },
   "outputs": [],
   "source": [
    "\n",
    "class BuildModel():\n",
    "    \"\"\"\n",
    "    Build a model. Arguments allow one to customise the hyper parameters\n",
    "    ATTRIBUTES :- \n",
    "    length - number of steps in time sequence to feed the rnn\n",
    "    layers_num - number of rnn layers in model (capped at 3)\n",
    "    layers_type - select \"LSTM\" or \"GRU\"\n",
    "    units - number of units in rnn layers\n",
    "    num_step_preds - number of steps/days in time to predict\n",
    "    dropout - dropout % to be applied to rnn units\n",
    "    g_filt - gaussian filter for smoothing. Default: no smoothing\n",
    "    batch_size - number of samples to feed model at a time.\n",
    "    patience - how many epochs to wait before stopping model after finding good score.\n",
    "    model_name - file name of model we save. must end in \".h5\" eg 'temp_model.h5'\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name, length=10, layers_num=1, layers_type='LSTM',\\\n",
    "                 units=50, dropout=0.0, g_filt=00.1, num_step_preds=1,\\\n",
    "                 epochs=8, batch_size=1, patience=5):\n",
    "        \n",
    "        #assertions for input\n",
    "        assert 0 < layers_num < 4, \"1 <= layers_num <= 3\"\n",
    "        assert layers_type in ['LSTM', 'GRU'], \"layers_type is LSTM or GRU\"\n",
    "        assert 0 <= dropout < 1, \"dropout must be float < 1\"\n",
    "        assert model_name[-3:] == '.h5', \"End model_name with '.h5'\"\n",
    "        \n",
    "        #initialise\n",
    "        self.model_name = model_name        \n",
    "        self.length = length\n",
    "        self.layers_num = layers_num\n",
    "        self.layers_type = layers_type\n",
    "        self.units = units\n",
    "        self.num_step_preds = num_step_preds\n",
    "        self.dropout = dropout\n",
    "        self.g_filt = g_filt\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.n_features = 1\n",
    "        \n",
    "        #callbacks\n",
    "        self.callbacks =[EarlyStopping(monitor='val_loss', patience=patience),\\\n",
    "                         ModelCheckpoint(self.model_name, monitor='val_loss',\\\n",
    "                                         save_best_only=True)]\n",
    "        \n",
    "        #BUILD MODEL\n",
    "        ##inputs\n",
    "        self.model = Sequential()\n",
    "        self.model.add(InputLayer(input_shape=(self.length, self.n_features)))\n",
    "        \n",
    "        ##add extra layers as required (or not if layers_num = 1)\n",
    "        for i in range(layers_num - 1):\n",
    "            self.model.add(eval('{}(units={}, dropout={}, return_sequences=True)'\\\n",
    "                .format(self.layers_type, self.units, self.dropout)))\n",
    "                \n",
    "        ##closing rnn layer (do not return squences)\n",
    "        self.model.add(eval('{}(units={}, dropout={})'\\\n",
    "                .format(self.layers_type, self.units, self.dropout)))\n",
    "            \n",
    "        ##Dense output\n",
    "        self.model.add(Dense(units=self.num_step_preds))\n",
    "                       \n",
    "        #compile model\n",
    "        self.model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "    \n",
    "    def setupData(self, series, val_days=450):\n",
    "        \"\"\"\n",
    "        splits data, scales data, creates generators for the model\n",
    "        \"\"\"\n",
    "        assert val_days > self.length , \"val_days must exceed lenght\"\n",
    "        \n",
    "        #split data into train and validation\n",
    "        self.train = series.iloc[:-val_days]\n",
    "        self.validation = series.iloc[-val_days:]\n",
    "        \n",
    "        #Apply smoothing filters  \n",
    "        self.train_smooth = \\\n",
    "             gaussian_filter1d(self.train, self.g_filt)\\\n",
    "                 .reshape(-1,1)\n",
    "            \n",
    "        self.validation_smooth = \\\n",
    "             gaussian_filter1d(self.validation, self.g_filt)\\\n",
    "                 .reshape(-1,1)\n",
    "\n",
    "        #create time series generators\n",
    "        self.generator = \\\n",
    "             TimeseriesGenerator(data=self.train_smooth,\\\n",
    "                                 targets=self.train_smooth,\\\n",
    "                                 length=self.length,\\\n",
    "                                 batch_size=self.batch_size)\n",
    "                 \n",
    "        self.val_generator = \\\n",
    "             TimeseriesGenerator(data=self.validation_smooth,\\\n",
    "                                 targets=self.validation_smooth,\\\n",
    "                                 length=self.length,\\\n",
    "                                 batch_size=self.batch_size)                 \n",
    "\n",
    "    def fitModel(self):\n",
    "        \"\"\"\n",
    "        Fits the model on your generators for training and validation sets.\n",
    "        EarlyStopping call back ends training if val_loss doesnt improve.\n",
    "        Record epoch metrics in a DataFrame.\n",
    "        \"\"\"\n",
    "        self.model.fit(self.generator, validation_data=self.val_generator,\\\n",
    "                       epochs=self.epochs, callbacks=self.callbacks)\n",
    "            \n",
    "        self.history = pd.DataFrame(self.model.history.history)\n",
    "        \n",
    "    def loadModel(self):\n",
    "        \"\"\"\n",
    "        Load a model instead of fitting a new one (uses model_name)\n",
    "        \"\"\"\n",
    "        self.model = tf.keras.models.load_model(self.model_name)\n",
    "            \n",
    "    def predAhead(self, days, series=None):\n",
    "        \"\"\"\n",
    "        Predicts a number of days ahead set by the user. Input your own\n",
    "        series or dont if you want to predict off of the validation set.\n",
    "        \"\"\"\n",
    "        assert self.num_step_preds == 1,\\\n",
    "            \"sorry, function not yet available for multi step models\"\n",
    "        \n",
    "        #use end of the validation set to project forward if no series given\n",
    "        if series is None:\n",
    "            series = self.validation\n",
    "        \n",
    "        #get end of the series to plug into the model\n",
    "        assert len(series) >= self.length,\\\n",
    "            \"series must be at least {} days\".format(self.length)\n",
    "            \n",
    "        series_cut = series.iloc[-self.length:].values.reshape(-1,1)\n",
    "            \n",
    "        #predict ahead by appending predictions and removing first values\n",
    "        pred_series = series_cut.reshape(1, self.length, self.n_features)\n",
    "\n",
    "        predictions = []\n",
    "        \n",
    "        for i in range(days):\n",
    "            pred = self.model.predict(pred_series)\n",
    "            pred_series = np.append(pred_series[:,1:,:], [pred], axis=1)\n",
    "            predictions.append(pred)\n",
    "                    \n",
    "        #convert to pandas series\n",
    "        predictions = np.array(predictions)\n",
    "        predictions = pd.Series(predictions.reshape(days))\n",
    "        predictions.index = self.validation.index[-days:] +\\\n",
    "                                 dt.timedelta(days=days)\n",
    "            \n",
    "        return predictions\n",
    "    \n",
    "    def plotPreds(self, predictions, test_series=None, run_up=None,\\\n",
    "                  ylabel='units'):\n",
    "        \"\"\"\n",
    "        plot the predictions of the model. plot them against another series\n",
    "        (test series). plot with with a run up leading to the pred period\n",
    "        (validation set).\n",
    "        \"\"\"\n",
    "        #set up figure\n",
    "        plt.figure(figsize=(10,6))\n",
    "        plt.ylabel(ylabel)\n",
    "        plt.xlabel('datetime')\n",
    "        \n",
    "        #plot lines\n",
    "        if run_up is None:\n",
    "            run_up = self.validation[-7:]\n",
    "            \n",
    "        if test_series is not None:\n",
    "            plt.plot(pd.concat([run_up, test_series[:1]]))\n",
    "            plt.plot(test_series)\n",
    "            \n",
    "        else:\n",
    "            plt.plot(run_up)\n",
    "            \n",
    "        #plot points\n",
    "        plt.scatter(predictions.index, predictions, edgecolors='k',\\\n",
    "                    label='predictions', c='#2ca02c', s=64)\n",
    "            \n",
    "        if test_series is not None:\n",
    "            plt.scatter(test_series.index, test_series, marker='X',\\\n",
    "                        edgecolors='k', label='test_data', c='#ff7f0e', s=200)\n",
    "                \n",
    "        plt.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "enCjdIxABB9a"
   },
   "source": [
    "##Define Functions for searching over the model's hyperparameters\n",
    "NOTE: Requires Pandas 1.2.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3zeq00CFI009"
   },
   "outputs": [],
   "source": [
    "#! pip install pandas==1.2.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ds7gfFqTdwG1"
   },
   "outputs": [],
   "source": [
    "def gridTableGen(length: list, layers_num: list, layers_type: list,\\\n",
    "                 units: list, g_filt: list):\n",
    "    \"\"\"returns table of every combo for the hyperparameters\"\"\"\n",
    "    \n",
    "    #get cross joins to acquire every combination\n",
    "    grid_table = pd.DataFrame(length).merge(\\\n",
    "                 pd.DataFrame(layers_num), how='cross').merge(\\\n",
    "                 pd.DataFrame(layers_type), how='cross').merge(\\\n",
    "                 pd.DataFrame(units), how='cross').merge(\\\n",
    "                 pd.DataFrame(g_filt), how='cross')  \n",
    "                                                          \n",
    "    grid_table.columns = \\\n",
    "        ['length', 'layers_num', 'layers_type', 'units', 'g_filt']\n",
    "        \n",
    "    return grid_table\n",
    "\n",
    "def gridSearch(grid_table, data):\n",
    "    \"\"\"searches through hyperparameters in grid_table to determine optimium model\"\"\"\n",
    "    #record time for file_name\n",
    "    time_now = str(round(time.time()))\n",
    "        \n",
    "    #make results table to append results onto\n",
    "    results_cols =\\\n",
    "        pd.DataFrame(columns=['loss', 'mae', 'val_loss', 'val_mae',\\\n",
    "                              'val_mae_og','epochs'])\n",
    "        \n",
    "    results_table = pd.concat([grid_table, results_cols], axis=1)\n",
    "    \n",
    "    #iterate through the table and fit the models\n",
    "    for i, row in grid_table.iterrows():\n",
    "      if i < 64:\n",
    "        continue\n",
    "      #input hyperparameters\n",
    "      print('\\nNow Training ({})\\n{}'.format(i, row.to_dict()))\n",
    "      grid_mod = \\\n",
    "          BuildModel(model_name='temp_model.h5', length=row['length'],\\\n",
    "                      layers_num=row['layers_num'], \\\n",
    "                      layers_type=row['layers_type'],units=row['units'],\\\n",
    "                      g_filt=row['g_filt'], num_step_preds=1,\\\n",
    "                      epochs=250, batch_size=10, patience=20)\n",
    "      \n",
    "      #setup data and train the model\n",
    "      grid_mod.setupData(data)\n",
    "      grid_mod.fitModel()\n",
    "      \n",
    "      #find best epoch (val_mae)\n",
    "      hist = grid_mod.history\n",
    "      best_epoch = hist[hist['val_mae'] == hist['val_mae'].min()]\\\n",
    "                    .iloc[:1]\n",
    "                    \n",
    "      #calculate val_mae in unsmoothed orginal units\n",
    "      best_model = tf.keras.models.load_model(grid_mod.model_name)\n",
    "      preds = best_model.predict(grid_mod.val_generator)\n",
    "      preds = pd.Series(preds[:,0],\\\n",
    "                  index = grid_mod.validation[grid_mod.length:].index)\n",
    "\n",
    "      val_mae_og = (preds - grid_mod.validation[grid_mod.length:]).abs()\\\n",
    "                    .mean()\n",
    "      \n",
    "      #update results table\n",
    "      results_table.loc[i, ['loss', 'mae', 'val_loss', 'val_mae']] =\\\n",
    "          best_epoch.values[0].round(4)\n",
    "      \n",
    "      results_table.loc[i, 'epochs'] = best_epoch.index[0]\n",
    "      results_table.loc[i, 'val_mae_og'] = val_mae_og\n",
    "      \n",
    "      #save to drive\n",
    "      results_table.to_csv('results_table_' + time_now + '.csv', index=False)\n",
    "        \n",
    "    return results_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BDvVUw88BdGc"
   },
   "source": [
    "##Use functions and class to optimise a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9353240,
     "status": "ok",
     "timestamp": 1621774411140,
     "user": {
      "displayName": "1990JAF",
      "photoUrl": "",
      "userId": "13601147066600634770"
     },
     "user_tz": -60
    },
    "id": "D504_ECw0s_h",
    "outputId": "611d55f1-a223-4d61-951b-d38a7b1f8081"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Now Training (64)\n",
      "{'length': 180, 'layers_num': 2, 'layers_type': 'LSTM', 'units': 40, 'g_filt': 1.0}\n",
      "Epoch 1/250\n",
      "177/177 [==============================] - 44s 71ms/step - loss: 75.5243 - mae: 7.0392 - val_loss: 10.5610 - val_mae: 2.6356\n",
      "Epoch 2/250\n",
      "177/177 [==============================] - 11s 64ms/step - loss: 14.4000 - mae: 2.8886 - val_loss: 4.4369 - val_mae: 1.6983\n",
      "Epoch 3/250\n",
      "177/177 [==============================] - 11s 65ms/step - loss: 6.2255 - mae: 1.8119 - val_loss: 3.9677 - val_mae: 1.6467\n",
      "Epoch 4/250\n",
      "177/177 [==============================] - 11s 65ms/step - loss: 4.0179 - mae: 1.4461 - val_loss: 1.8307 - val_mae: 1.0396\n",
      "Epoch 5/250\n",
      "177/177 [==============================] - 11s 64ms/step - loss: 2.8102 - mae: 1.1635 - val_loss: 1.2364 - val_mae: 0.8672\n",
      "Epoch 6/250\n",
      "177/177 [==============================] - 11s 64ms/step - loss: 2.0219 - mae: 0.9772 - val_loss: 0.9363 - val_mae: 0.7472\n",
      "Epoch 7/250\n",
      "177/177 [==============================] - 11s 64ms/step - loss: 1.3514 - mae: 0.8237 - val_loss: 0.6453 - val_mae: 0.6260\n",
      "Epoch 8/250\n",
      "177/177 [==============================] - 11s 64ms/step - loss: 0.8585 - mae: 0.6642 - val_loss: 0.7103 - val_mae: 0.6677\n",
      "Epoch 9/250\n",
      "177/177 [==============================] - 11s 64ms/step - loss: 0.8381 - mae: 0.6236 - val_loss: 0.4259 - val_mae: 0.5290\n",
      "Epoch 10/250\n",
      "177/177 [==============================] - 11s 64ms/step - loss: 0.6498 - mae: 0.5376 - val_loss: 0.5545 - val_mae: 0.6341\n",
      "Epoch 11/250\n",
      "177/177 [==============================] - 11s 64ms/step - loss: 0.4286 - mae: 0.4928 - val_loss: 0.3144 - val_mae: 0.4314\n",
      "Epoch 12/250\n",
      "177/177 [==============================] - 11s 64ms/step - loss: 0.4549 - mae: 0.4644 - val_loss: 0.2691 - val_mae: 0.4032\n",
      "Epoch 13/250\n",
      "177/177 [==============================] - 11s 64ms/step - loss: 0.3735 - mae: 0.4526 - val_loss: 0.2561 - val_mae: 0.3895\n",
      "Epoch 14/250\n",
      "177/177 [==============================] - 11s 64ms/step - loss: 0.3598 - mae: 0.4302 - val_loss: 0.2980 - val_mae: 0.4224\n",
      "Epoch 15/250\n",
      "177/177 [==============================] - 11s 64ms/step - loss: 0.3889 - mae: 0.4576 - val_loss: 0.2344 - val_mae: 0.3783\n",
      "Epoch 16/250\n",
      "177/177 [==============================] - 11s 64ms/step - loss: 0.2858 - mae: 0.3984 - val_loss: 0.2695 - val_mae: 0.4146\n",
      "Epoch 17/250\n",
      "177/177 [==============================] - 11s 64ms/step - loss: 0.2887 - mae: 0.4055 - val_loss: 0.2863 - val_mae: 0.4243\n",
      "Epoch 18/250\n",
      "177/177 [==============================] - 11s 64ms/step - loss: 0.2398 - mae: 0.3661 - val_loss: 0.2178 - val_mae: 0.3630\n",
      "Epoch 19/250\n",
      "177/177 [==============================] - 11s 64ms/step - loss: 0.2706 - mae: 0.3819 - val_loss: 0.2071 - val_mae: 0.3533\n",
      "Epoch 20/250\n",
      "177/177 [==============================] - 11s 63ms/step - loss: 0.2654 - mae: 0.3955 - val_loss: 0.2481 - val_mae: 0.4009\n",
      "Epoch 21/250\n",
      "177/177 [==============================] - 11s 64ms/step - loss: 0.2302 - mae: 0.3689 - val_loss: 0.2722 - val_mae: 0.4195\n",
      "Epoch 22/250\n",
      "177/177 [==============================] - 11s 64ms/step - loss: 0.2773 - mae: 0.4043 - val_loss: 0.1959 - val_mae: 0.3411\n",
      "Epoch 23/250\n",
      "177/177 [==============================] - 11s 64ms/step - loss: 0.2086 - mae: 0.3536 - val_loss: 0.2604 - val_mae: 0.3967\n",
      "Epoch 24/250\n",
      "177/177 [==============================] - 11s 64ms/step - loss: 0.2360 - mae: 0.3796 - val_loss: 0.2088 - val_mae: 0.3552\n",
      "Epoch 25/250\n",
      "177/177 [==============================] - 11s 65ms/step - loss: 0.2120 - mae: 0.3586 - val_loss: 0.1929 - val_mae: 0.3479\n",
      "Epoch 26/250\n",
      "177/177 [==============================] - 12s 65ms/step - loss: 0.2022 - mae: 0.3628 - val_loss: 0.4049 - val_mae: 0.5170\n",
      "Epoch 27/250\n",
      "177/177 [==============================] - 11s 64ms/step - loss: 0.2439 - mae: 0.3965 - val_loss: 0.2051 - val_mae: 0.3517\n",
      "Epoch 28/250\n",
      "177/177 [==============================] - 11s 65ms/step - loss: 0.2268 - mae: 0.3750 - val_loss: 0.1881 - val_mae: 0.3441\n",
      "Epoch 29/250\n",
      "177/177 [==============================] - 12s 65ms/step - loss: 0.2046 - mae: 0.3556 - val_loss: 0.3601 - val_mae: 0.4987\n",
      "Epoch 30/250\n",
      "177/177 [==============================] - 12s 66ms/step - loss: 0.2398 - mae: 0.3841 - val_loss: 0.1909 - val_mae: 0.3438\n",
      "Epoch 31/250\n",
      "177/177 [==============================] - 12s 66ms/step - loss: 0.1986 - mae: 0.3554 - val_loss: 0.2024 - val_mae: 0.3538\n",
      "Epoch 32/250\n",
      "177/177 [==============================] - 12s 66ms/step - loss: 0.2133 - mae: 0.3687 - val_loss: 0.3073 - val_mae: 0.4522\n",
      "Epoch 33/250\n",
      "177/177 [==============================] - 12s 66ms/step - loss: 0.2438 - mae: 0.3946 - val_loss: 0.1814 - val_mae: 0.3340\n",
      "Epoch 34/250\n",
      "177/177 [==============================] - 12s 65ms/step - loss: 0.1885 - mae: 0.3432 - val_loss: 0.2190 - val_mae: 0.3621\n",
      "Epoch 35/250\n",
      "177/177 [==============================] - 12s 65ms/step - loss: 0.1905 - mae: 0.3438 - val_loss: 0.1999 - val_mae: 0.3535\n",
      "Epoch 36/250\n",
      "177/177 [==============================] - 12s 66ms/step - loss: 0.1741 - mae: 0.3325 - val_loss: 0.3015 - val_mae: 0.4313\n",
      "Epoch 37/250\n",
      "177/177 [==============================] - 12s 65ms/step - loss: 0.1911 - mae: 0.3460 - val_loss: 0.1980 - val_mae: 0.3478\n",
      "Epoch 38/250\n",
      "177/177 [==============================] - 12s 65ms/step - loss: 0.1952 - mae: 0.3548 - val_loss: 0.1928 - val_mae: 0.3411\n",
      "Epoch 39/250\n",
      "177/177 [==============================] - 12s 65ms/step - loss: 0.1994 - mae: 0.3510 - val_loss: 0.1839 - val_mae: 0.3392\n",
      "Epoch 40/250\n",
      "177/177 [==============================] - 12s 66ms/step - loss: 0.2035 - mae: 0.3586 - val_loss: 0.1922 - val_mae: 0.3517\n",
      "Epoch 41/250\n",
      "177/177 [==============================] - 12s 65ms/step - loss: 0.1876 - mae: 0.3413 - val_loss: 0.2061 - val_mae: 0.3605\n",
      "Epoch 42/250\n",
      "177/177 [==============================] - 12s 65ms/step - loss: 0.1865 - mae: 0.3435 - val_loss: 0.2212 - val_mae: 0.3767\n",
      "Epoch 43/250\n",
      "177/177 [==============================] - 11s 65ms/step - loss: 0.2270 - mae: 0.3762 - val_loss: 0.2262 - val_mae: 0.3826\n",
      "Epoch 44/250\n",
      "177/177 [==============================] - 12s 65ms/step - loss: 0.1900 - mae: 0.3488 - val_loss: 0.1751 - val_mae: 0.3315\n",
      "Epoch 45/250\n",
      "177/177 [==============================] - 12s 66ms/step - loss: 0.1880 - mae: 0.3512 - val_loss: 0.2165 - val_mae: 0.3706\n",
      "Epoch 46/250\n",
      "177/177 [==============================] - 12s 66ms/step - loss: 0.1788 - mae: 0.3374 - val_loss: 0.2004 - val_mae: 0.3530\n",
      "Epoch 47/250\n",
      "177/177 [==============================] - 12s 65ms/step - loss: 0.1716 - mae: 0.3311 - val_loss: 0.2483 - val_mae: 0.3962\n",
      "Epoch 48/250\n",
      "177/177 [==============================] - 12s 66ms/step - loss: 0.1934 - mae: 0.3524 - val_loss: 0.1846 - val_mae: 0.3377\n",
      "Epoch 49/250\n",
      "177/177 [==============================] - 12s 66ms/step - loss: 0.1747 - mae: 0.3280 - val_loss: 0.1957 - val_mae: 0.3485\n",
      "Epoch 50/250\n",
      "177/177 [==============================] - 12s 65ms/step - loss: 0.1741 - mae: 0.3364 - val_loss: 0.4190 - val_mae: 0.5473\n",
      "Epoch 51/250\n",
      "177/177 [==============================] - 12s 65ms/step - loss: 0.2085 - mae: 0.3632 - val_loss: 0.1844 - val_mae: 0.3375\n",
      "Epoch 52/250\n",
      "177/177 [==============================] - 12s 66ms/step - loss: 0.1834 - mae: 0.3329 - val_loss: 0.1777 - val_mae: 0.3317\n",
      "Epoch 53/250\n",
      "177/177 [==============================] - 12s 65ms/step - loss: 0.1635 - mae: 0.3220 - val_loss: 0.2002 - val_mae: 0.3464\n",
      "Epoch 54/250\n",
      "177/177 [==============================] - 12s 66ms/step - loss: 0.1824 - mae: 0.3465 - val_loss: 0.1791 - val_mae: 0.3295\n",
      "Epoch 55/250\n",
      "177/177 [==============================] - 12s 65ms/step - loss: 0.1828 - mae: 0.3495 - val_loss: 0.1831 - val_mae: 0.3352\n",
      "Epoch 56/250\n",
      "177/177 [==============================] - 12s 66ms/step - loss: 0.1780 - mae: 0.3390 - val_loss: 0.2056 - val_mae: 0.3545\n",
      "Epoch 57/250\n",
      "177/177 [==============================] - 12s 65ms/step - loss: 0.1935 - mae: 0.3562 - val_loss: 0.2822 - val_mae: 0.4239\n",
      "Epoch 58/250\n",
      "177/177 [==============================] - 12s 66ms/step - loss: 0.1954 - mae: 0.3501 - val_loss: 0.2262 - val_mae: 0.3857\n",
      "Epoch 59/250\n",
      "177/177 [==============================] - 12s 66ms/step - loss: 0.1761 - mae: 0.3363 - val_loss: 0.2265 - val_mae: 0.3743\n",
      "Epoch 60/250\n",
      "177/177 [==============================] - 12s 65ms/step - loss: 0.1937 - mae: 0.3457 - val_loss: 0.3563 - val_mae: 0.4938\n",
      "Epoch 61/250\n",
      "177/177 [==============================] - 12s 65ms/step - loss: 0.1892 - mae: 0.3473 - val_loss: 0.1801 - val_mae: 0.3324\n",
      "Epoch 62/250\n",
      "177/177 [==============================] - 12s 65ms/step - loss: 0.1771 - mae: 0.3355 - val_loss: 0.2140 - val_mae: 0.3684\n",
      "Epoch 63/250\n",
      "177/177 [==============================] - 12s 65ms/step - loss: 0.1953 - mae: 0.3464 - val_loss: 0.2554 - val_mae: 0.4131\n",
      "Epoch 64/250\n",
      "177/177 [==============================] - 11s 65ms/step - loss: 0.1960 - mae: 0.3553 - val_loss: 0.1808 - val_mae: 0.3357\n",
      "\n",
      "Now Training (65)\n",
      "{'length': 180, 'layers_num': 2, 'layers_type': 'LSTM', 'units': 40, 'g_filt': 1.5}\n",
      "Epoch 1/250\n",
      "177/177 [==============================] - 16s 70ms/step - loss: 75.9123 - mae: 6.9616 - val_loss: 10.1220 - val_mae: 2.5105\n",
      "Epoch 2/250\n",
      "177/177 [==============================] - 12s 65ms/step - loss: 14.5124 - mae: 2.9044 - val_loss: 3.5587 - val_mae: 1.4663\n",
      "Epoch 3/250\n",
      "177/177 [==============================] - 12s 65ms/step - loss: 5.5878 - mae: 1.7518 - val_loss: 2.2712 - val_mae: 1.1799\n",
      "Epoch 4/250\n",
      "177/177 [==============================] - 12s 65ms/step - loss: 3.9473 - mae: 1.4141 - val_loss: 1.4763 - val_mae: 0.9358\n",
      "Epoch 5/250\n",
      "177/177 [==============================] - 12s 66ms/step - loss: 2.3368 - mae: 1.0591 - val_loss: 1.2118 - val_mae: 0.8840\n",
      "Epoch 6/250\n",
      "177/177 [==============================] - 12s 66ms/step - loss: 1.2221 - mae: 0.7807 - val_loss: 0.5507 - val_mae: 0.5652\n",
      "Epoch 7/250\n",
      "177/177 [==============================] - 12s 65ms/step - loss: 0.8515 - mae: 0.6183 - val_loss: 0.4622 - val_mae: 0.5246\n",
      "Epoch 8/250\n",
      "177/177 [==============================] - 12s 65ms/step - loss: 0.6852 - mae: 0.5197 - val_loss: 0.5272 - val_mae: 0.5984\n",
      "Epoch 9/250\n",
      "177/177 [==============================] - 12s 65ms/step - loss: 0.6375 - mae: 0.5116 - val_loss: 0.2721 - val_mae: 0.3961\n",
      "Epoch 10/250\n",
      "177/177 [==============================] - 12s 66ms/step - loss: 0.3997 - mae: 0.4189 - val_loss: 0.2416 - val_mae: 0.3791\n",
      "Epoch 11/250\n",
      "177/177 [==============================] - 11s 65ms/step - loss: 0.3060 - mae: 0.3631 - val_loss: 0.2866 - val_mae: 0.4352\n",
      "Epoch 12/250\n",
      "177/177 [==============================] - 11s 65ms/step - loss: 0.3354 - mae: 0.3677 - val_loss: 0.1274 - val_mae: 0.2695\n",
      "Epoch 13/250\n",
      "177/177 [==============================] - 11s 64ms/step - loss: 0.1727 - mae: 0.2750 - val_loss: 0.1175 - val_mae: 0.2540\n",
      "Epoch 14/250\n",
      "177/177 [==============================] - 11s 65ms/step - loss: 0.1945 - mae: 0.2930 - val_loss: 0.1002 - val_mae: 0.2471\n",
      "Epoch 15/250\n",
      "177/177 [==============================] - 11s 65ms/step - loss: 0.1142 - mae: 0.2284 - val_loss: 0.0996 - val_mae: 0.2280\n",
      "Epoch 16/250\n",
      "177/177 [==============================] - 11s 65ms/step - loss: 0.0838 - mae: 0.2033 - val_loss: 0.0754 - val_mae: 0.2135\n",
      "Epoch 17/250\n",
      "177/177 [==============================] - 11s 64ms/step - loss: 0.1005 - mae: 0.2313 - val_loss: 0.1336 - val_mae: 0.3102\n",
      "Epoch 18/250\n",
      "177/177 [==============================] - 11s 64ms/step - loss: 0.1202 - mae: 0.2455 - val_loss: 0.0613 - val_mae: 0.1838\n",
      "Epoch 19/250\n",
      "177/177 [==============================] - 11s 64ms/step - loss: 0.1111 - mae: 0.2146 - val_loss: 0.0735 - val_mae: 0.1942\n",
      "Epoch 20/250\n",
      "177/177 [==============================] - 11s 65ms/step - loss: 0.0901 - mae: 0.2066 - val_loss: 0.0470 - val_mae: 0.1709\n",
      "Epoch 21/250\n",
      "177/177 [==============================] - 11s 65ms/step - loss: 0.0937 - mae: 0.2224 - val_loss: 0.0481 - val_mae: 0.1794\n",
      "Epoch 22/250\n",
      "177/177 [==============================] - 12s 65ms/step - loss: 0.0746 - mae: 0.1881 - val_loss: 0.1290 - val_mae: 0.3064\n",
      "Epoch 23/250\n",
      "177/177 [==============================] - 11s 65ms/step - loss: 0.0626 - mae: 0.1818 - val_loss: 0.0795 - val_mae: 0.2346\n",
      "Epoch 24/250\n",
      "177/177 [==============================] - 11s 65ms/step - loss: 0.0544 - mae: 0.1685 - val_loss: 0.0243 - val_mae: 0.1142\n",
      "Epoch 25/250\n",
      "177/177 [==============================] - 11s 65ms/step - loss: 0.0343 - mae: 0.1298 - val_loss: 0.0802 - val_mae: 0.2487\n",
      "Epoch 26/250\n",
      "177/177 [==============================] - 11s 64ms/step - loss: 0.0496 - mae: 0.1626 - val_loss: 0.0305 - val_mae: 0.1286\n",
      "Epoch 27/250\n",
      "177/177 [==============================] - 11s 65ms/step - loss: 0.0342 - mae: 0.1306 - val_loss: 0.0487 - val_mae: 0.1716\n",
      "Epoch 28/250\n",
      "177/177 [==============================] - 12s 65ms/step - loss: 0.0367 - mae: 0.1436 - val_loss: 0.0246 - val_mae: 0.1213\n",
      "Epoch 29/250\n",
      "177/177 [==============================] - 11s 64ms/step - loss: 0.0286 - mae: 0.1183 - val_loss: 0.0357 - val_mae: 0.1499\n",
      "Epoch 30/250\n",
      "177/177 [==============================] - 11s 65ms/step - loss: 0.0445 - mae: 0.1565 - val_loss: 0.0229 - val_mae: 0.1051\n",
      "Epoch 31/250\n",
      "177/177 [==============================] - 12s 65ms/step - loss: 0.0280 - mae: 0.1214 - val_loss: 0.0406 - val_mae: 0.1733\n",
      "Epoch 32/250\n",
      "177/177 [==============================] - 11s 65ms/step - loss: 0.0221 - mae: 0.1078 - val_loss: 0.0224 - val_mae: 0.1060\n",
      "Epoch 33/250\n",
      "177/177 [==============================] - 11s 65ms/step - loss: 0.0295 - mae: 0.1220 - val_loss: 0.0384 - val_mae: 0.1712\n",
      "Epoch 34/250\n",
      "177/177 [==============================] - 11s 65ms/step - loss: 0.0853 - mae: 0.2171 - val_loss: 0.0207 - val_mae: 0.1083\n",
      "Epoch 35/250\n",
      "177/177 [==============================] - 12s 65ms/step - loss: 0.0158 - mae: 0.0949 - val_loss: 0.0145 - val_mae: 0.0931\n",
      "Epoch 36/250\n",
      "177/177 [==============================] - 11s 65ms/step - loss: 0.0394 - mae: 0.1430 - val_loss: 0.0121 - val_mae: 0.0848\n",
      "Epoch 37/250\n",
      "177/177 [==============================] - 11s 65ms/step - loss: 0.0224 - mae: 0.1116 - val_loss: 0.0109 - val_mae: 0.0809\n",
      "Epoch 38/250\n",
      "177/177 [==============================] - 11s 65ms/step - loss: 0.0191 - mae: 0.0952 - val_loss: 0.2369 - val_mae: 0.4637\n",
      "Epoch 39/250\n",
      "177/177 [==============================] - 11s 65ms/step - loss: 0.0540 - mae: 0.1666 - val_loss: 0.0126 - val_mae: 0.0853\n",
      "Epoch 40/250\n",
      "177/177 [==============================] - 11s 65ms/step - loss: 0.0232 - mae: 0.1111 - val_loss: 0.0150 - val_mae: 0.1011\n",
      "Epoch 41/250\n",
      "177/177 [==============================] - 11s 64ms/step - loss: 0.0329 - mae: 0.1354 - val_loss: 0.0124 - val_mae: 0.0875\n",
      "Epoch 42/250\n",
      "177/177 [==============================] - 11s 64ms/step - loss: 0.0231 - mae: 0.1145 - val_loss: 0.0108 - val_mae: 0.0806\n",
      "Epoch 43/250\n",
      "177/177 [==============================] - 11s 65ms/step - loss: 0.0148 - mae: 0.0921 - val_loss: 0.0225 - val_mae: 0.1128\n",
      "Epoch 44/250\n",
      "177/177 [==============================] - 11s 65ms/step - loss: 0.0327 - mae: 0.1354 - val_loss: 0.0278 - val_mae: 0.1440\n",
      "Epoch 45/250\n",
      "177/177 [==============================] - 11s 65ms/step - loss: 0.0417 - mae: 0.1565 - val_loss: 0.0167 - val_mae: 0.1094\n",
      "Epoch 46/250\n",
      "177/177 [==============================] - 11s 65ms/step - loss: 0.0306 - mae: 0.1238 - val_loss: 0.0123 - val_mae: 0.0870\n",
      "Epoch 47/250\n",
      "177/177 [==============================] - 12s 65ms/step - loss: 0.0115 - mae: 0.0834 - val_loss: 0.0173 - val_mae: 0.1030\n",
      "Epoch 48/250\n",
      "177/177 [==============================] - 12s 65ms/step - loss: 0.0140 - mae: 0.0877 - val_loss: 0.0739 - val_mae: 0.2338\n",
      "Epoch 49/250\n",
      "177/177 [==============================] - 11s 64ms/step - loss: 0.0337 - mae: 0.1391 - val_loss: 0.0112 - val_mae: 0.0865\n",
      "Epoch 50/250\n",
      "177/177 [==============================] - 11s 65ms/step - loss: 0.0155 - mae: 0.0934 - val_loss: 0.0100 - val_mae: 0.0798\n",
      "Epoch 51/250\n",
      "177/177 [==============================] - 12s 65ms/step - loss: 0.0091 - mae: 0.0727 - val_loss: 0.0379 - val_mae: 0.1602\n",
      "Epoch 52/250\n",
      "177/177 [==============================] - 11s 64ms/step - loss: 0.0263 - mae: 0.1279 - val_loss: 0.0120 - val_mae: 0.0832\n",
      "Epoch 53/250\n",
      "177/177 [==============================] - 12s 65ms/step - loss: 0.0130 - mae: 0.0811 - val_loss: 0.0150 - val_mae: 0.1015\n",
      "Epoch 54/250\n",
      "177/177 [==============================] - 11s 65ms/step - loss: 0.0211 - mae: 0.1082 - val_loss: 0.0084 - val_mae: 0.0724\n",
      "Epoch 55/250\n",
      "177/177 [==============================] - 11s 64ms/step - loss: 0.0109 - mae: 0.0810 - val_loss: 0.0475 - val_mae: 0.1963\n",
      "Epoch 56/250\n",
      "177/177 [==============================] - 11s 65ms/step - loss: 0.0515 - mae: 0.1799 - val_loss: 0.0625 - val_mae: 0.2166\n",
      "Epoch 57/250\n",
      "177/177 [==============================] - 11s 64ms/step - loss: 0.1021 - mae: 0.2485 - val_loss: 0.0409 - val_mae: 0.1758\n",
      "Epoch 58/250\n",
      "177/177 [==============================] - 11s 65ms/step - loss: 0.0278 - mae: 0.1290 - val_loss: 0.0145 - val_mae: 0.0930\n",
      "Epoch 59/250\n",
      "177/177 [==============================] - 11s 64ms/step - loss: 0.0280 - mae: 0.1254 - val_loss: 0.0110 - val_mae: 0.0824\n",
      "Epoch 60/250\n",
      "177/177 [==============================] - 11s 65ms/step - loss: 0.0229 - mae: 0.1147 - val_loss: 0.0199 - val_mae: 0.1088\n",
      "Epoch 61/250\n",
      "177/177 [==============================] - 11s 65ms/step - loss: 0.0254 - mae: 0.1250 - val_loss: 0.7062 - val_mae: 0.8275\n",
      "Epoch 62/250\n",
      "177/177 [==============================] - 11s 65ms/step - loss: 0.0748 - mae: 0.1888 - val_loss: 0.0192 - val_mae: 0.1171\n",
      "Epoch 63/250\n",
      "177/177 [==============================] - 11s 65ms/step - loss: 0.0131 - mae: 0.0885 - val_loss: 0.0095 - val_mae: 0.0750\n",
      "Epoch 64/250\n",
      "177/177 [==============================] - 11s 64ms/step - loss: 0.0172 - mae: 0.1052 - val_loss: 0.0085 - val_mae: 0.0733\n",
      "Epoch 65/250\n",
      "177/177 [==============================] - 12s 66ms/step - loss: 0.0115 - mae: 0.0836 - val_loss: 0.0082 - val_mae: 0.0729\n",
      "Epoch 66/250\n",
      "177/177 [==============================] - 11s 65ms/step - loss: 0.0174 - mae: 0.1030 - val_loss: 0.0091 - val_mae: 0.0756\n",
      "Epoch 67/250\n",
      "177/177 [==============================] - 12s 65ms/step - loss: 0.0263 - mae: 0.1262 - val_loss: 0.0134 - val_mae: 0.0907\n",
      "Epoch 68/250\n",
      "177/177 [==============================] - 12s 66ms/step - loss: 0.0253 - mae: 0.1227 - val_loss: 0.0072 - val_mae: 0.0664\n",
      "Epoch 69/250\n",
      "177/177 [==============================] - 12s 66ms/step - loss: 0.0099 - mae: 0.0759 - val_loss: 0.0235 - val_mae: 0.1293\n",
      "Epoch 70/250\n",
      "177/177 [==============================] - 12s 66ms/step - loss: 0.0152 - mae: 0.0988 - val_loss: 0.0169 - val_mae: 0.1053\n",
      "Epoch 71/250\n",
      "177/177 [==============================] - 12s 65ms/step - loss: 0.0254 - mae: 0.1199 - val_loss: 0.0084 - val_mae: 0.0723\n",
      "Epoch 72/250\n",
      "177/177 [==============================] - 11s 65ms/step - loss: 0.0168 - mae: 0.1007 - val_loss: 0.0122 - val_mae: 0.0866\n",
      "Epoch 73/250\n",
      "177/177 [==============================] - 11s 65ms/step - loss: 0.0130 - mae: 0.0895 - val_loss: 0.0216 - val_mae: 0.1236\n",
      "Epoch 74/250\n",
      "177/177 [==============================] - 12s 65ms/step - loss: 0.0157 - mae: 0.1006 - val_loss: 0.0108 - val_mae: 0.0839\n",
      "Epoch 75/250\n",
      "177/177 [==============================] - 12s 65ms/step - loss: 0.0141 - mae: 0.0932 - val_loss: 0.0137 - val_mae: 0.0906\n",
      "Epoch 76/250\n",
      "177/177 [==============================] - 11s 65ms/step - loss: 0.0353 - mae: 0.1465 - val_loss: 0.0063 - val_mae: 0.0619\n",
      "Epoch 77/250\n",
      "177/177 [==============================] - 12s 65ms/step - loss: 0.0084 - mae: 0.0714 - val_loss: 0.0266 - val_mae: 0.1384\n",
      "Epoch 78/250\n",
      "177/177 [==============================] - 11s 65ms/step - loss: 0.0338 - mae: 0.1446 - val_loss: 0.0319 - val_mae: 0.1607\n",
      "Epoch 79/250\n",
      "177/177 [==============================] - 11s 65ms/step - loss: 0.0182 - mae: 0.1035 - val_loss: 0.0200 - val_mae: 0.1184\n",
      "Epoch 80/250\n",
      "177/177 [==============================] - 12s 65ms/step - loss: 0.0135 - mae: 0.0898 - val_loss: 0.0085 - val_mae: 0.0743\n",
      "Epoch 81/250\n",
      "177/177 [==============================] - 11s 65ms/step - loss: 0.0262 - mae: 0.1250 - val_loss: 0.0313 - val_mae: 0.1408\n",
      "Epoch 82/250\n",
      "177/177 [==============================] - 12s 65ms/step - loss: 0.0197 - mae: 0.1057 - val_loss: 0.0102 - val_mae: 0.0752\n",
      "Epoch 83/250\n",
      "177/177 [==============================] - 11s 65ms/step - loss: 0.0197 - mae: 0.1054 - val_loss: 0.0172 - val_mae: 0.0954\n",
      "Epoch 84/250\n",
      "177/177 [==============================] - 11s 65ms/step - loss: 0.0265 - mae: 0.1238 - val_loss: 0.0100 - val_mae: 0.0767\n",
      "Epoch 85/250\n",
      "177/177 [==============================] - 11s 65ms/step - loss: 0.0110 - mae: 0.0729 - val_loss: 0.0108 - val_mae: 0.0833\n",
      "Epoch 86/250\n",
      "177/177 [==============================] - 12s 65ms/step - loss: 0.0311 - mae: 0.1381 - val_loss: 0.0063 - val_mae: 0.0617\n",
      "Epoch 87/250\n",
      "177/177 [==============================] - 11s 65ms/step - loss: 0.0118 - mae: 0.0834 - val_loss: 0.0101 - val_mae: 0.0809\n",
      "Epoch 88/250\n",
      "177/177 [==============================] - 11s 65ms/step - loss: 0.0105 - mae: 0.0776 - val_loss: 0.0446 - val_mae: 0.1871\n",
      "Epoch 89/250\n",
      "177/177 [==============================] - 11s 65ms/step - loss: 0.1299 - mae: 0.2830 - val_loss: 0.0092 - val_mae: 0.0787\n",
      "Epoch 90/250\n",
      "177/177 [==============================] - 11s 65ms/step - loss: 0.0099 - mae: 0.0797 - val_loss: 0.0324 - val_mae: 0.1579\n",
      "Epoch 91/250\n",
      "177/177 [==============================] - 12s 65ms/step - loss: 0.0118 - mae: 0.0857 - val_loss: 0.0089 - val_mae: 0.0762\n",
      "Epoch 92/250\n",
      "177/177 [==============================] - 11s 65ms/step - loss: 0.0091 - mae: 0.0756 - val_loss: 0.0066 - val_mae: 0.0647\n",
      "Epoch 93/250\n",
      "177/177 [==============================] - 12s 65ms/step - loss: 0.0093 - mae: 0.0716 - val_loss: 0.0076 - val_mae: 0.0715\n",
      "Epoch 94/250\n",
      "177/177 [==============================] - 12s 65ms/step - loss: 0.0058 - mae: 0.0595 - val_loss: 0.0061 - val_mae: 0.0595\n",
      "Epoch 95/250\n",
      "177/177 [==============================] - 12s 65ms/step - loss: 0.0148 - mae: 0.0887 - val_loss: 0.0062 - val_mae: 0.0632\n",
      "Epoch 96/250\n",
      "177/177 [==============================] - 12s 66ms/step - loss: 0.0350 - mae: 0.1318 - val_loss: 0.0161 - val_mae: 0.1070\n",
      "Epoch 97/250\n",
      "177/177 [==============================] - 12s 66ms/step - loss: 0.0094 - mae: 0.0748 - val_loss: 0.0085 - val_mae: 0.0716\n",
      "Epoch 98/250\n",
      "177/177 [==============================] - 12s 65ms/step - loss: 0.0163 - mae: 0.0952 - val_loss: 0.0076 - val_mae: 0.0677\n",
      "Epoch 99/250\n",
      "177/177 [==============================] - 12s 65ms/step - loss: 0.0092 - mae: 0.0753 - val_loss: 0.0058 - val_mae: 0.0587\n",
      "Epoch 100/250\n",
      "177/177 [==============================] - 11s 65ms/step - loss: 0.0071 - mae: 0.0656 - val_loss: 0.0060 - val_mae: 0.0609\n",
      "Epoch 101/250\n",
      "177/177 [==============================] - 12s 65ms/step - loss: 0.0101 - mae: 0.0771 - val_loss: 0.0068 - val_mae: 0.0646\n",
      "Epoch 102/250\n",
      "177/177 [==============================] - 12s 65ms/step - loss: 0.0087 - mae: 0.0754 - val_loss: 0.0112 - val_mae: 0.0823\n",
      "Epoch 103/250\n",
      "177/177 [==============================] - 12s 65ms/step - loss: 0.0222 - mae: 0.1155 - val_loss: 0.0107 - val_mae: 0.0803\n",
      "Epoch 104/250\n",
      "177/177 [==============================] - 12s 65ms/step - loss: 0.0179 - mae: 0.1032 - val_loss: 0.0078 - val_mae: 0.0696\n",
      "Epoch 105/250\n",
      "177/177 [==============================] - 12s 65ms/step - loss: 0.0106 - mae: 0.0812 - val_loss: 0.0095 - val_mae: 0.0821\n",
      "Epoch 106/250\n",
      "177/177 [==============================] - 12s 65ms/step - loss: 0.0102 - mae: 0.0754 - val_loss: 0.0077 - val_mae: 0.0681\n",
      "Epoch 107/250\n",
      "177/177 [==============================] - 12s 65ms/step - loss: 0.0073 - mae: 0.0678 - val_loss: 0.0056 - val_mae: 0.0584\n",
      "Epoch 108/250\n",
      "177/177 [==============================] - 12s 65ms/step - loss: 0.0087 - mae: 0.0710 - val_loss: 0.0073 - val_mae: 0.0678\n",
      "Epoch 109/250\n",
      "177/177 [==============================] - 12s 65ms/step - loss: 0.0108 - mae: 0.0805 - val_loss: 0.0057 - val_mae: 0.0587\n",
      "Epoch 110/250\n",
      "177/177 [==============================] - 12s 65ms/step - loss: 0.0132 - mae: 0.0889 - val_loss: 0.0119 - val_mae: 0.0919\n",
      "Epoch 111/250\n",
      "177/177 [==============================] - 12s 65ms/step - loss: 0.0075 - mae: 0.0686 - val_loss: 0.0060 - val_mae: 0.0614\n",
      "Epoch 112/250\n",
      "177/177 [==============================] - 12s 65ms/step - loss: 0.0072 - mae: 0.0670 - val_loss: 0.0105 - val_mae: 0.0800\n",
      "Epoch 113/250\n",
      "177/177 [==============================] - 12s 66ms/step - loss: 0.0159 - mae: 0.0974 - val_loss: 0.0239 - val_mae: 0.1284\n",
      "Epoch 114/250\n",
      "177/177 [==============================] - 11s 65ms/step - loss: 0.0145 - mae: 0.0958 - val_loss: 0.0050 - val_mae: 0.0559\n",
      "Epoch 115/250\n",
      "177/177 [==============================] - 12s 66ms/step - loss: 0.0108 - mae: 0.0801 - val_loss: 0.0109 - val_mae: 0.0856\n",
      "Epoch 116/250\n",
      "177/177 [==============================] - 11s 65ms/step - loss: 0.0128 - mae: 0.0862 - val_loss: 0.0129 - val_mae: 0.0886\n",
      "Epoch 117/250\n",
      "177/177 [==============================] - 12s 65ms/step - loss: 0.0118 - mae: 0.0882 - val_loss: 0.0059 - val_mae: 0.0607\n",
      "Epoch 118/250\n",
      "177/177 [==============================] - 12s 66ms/step - loss: 0.0044 - mae: 0.0524 - val_loss: 0.0051 - val_mae: 0.0558\n",
      "Epoch 119/250\n",
      "177/177 [==============================] - 12s 66ms/step - loss: 0.0126 - mae: 0.0843 - val_loss: 0.0090 - val_mae: 0.0759\n",
      "Epoch 120/250\n",
      "177/177 [==============================] - 12s 66ms/step - loss: 0.0118 - mae: 0.0848 - val_loss: 0.0094 - val_mae: 0.0742\n",
      "Epoch 121/250\n",
      "177/177 [==============================] - 12s 65ms/step - loss: 0.0081 - mae: 0.0708 - val_loss: 0.0099 - val_mae: 0.0834\n",
      "Epoch 122/250\n",
      "177/177 [==============================] - 12s 65ms/step - loss: 0.3847 - mae: 0.2533 - val_loss: 0.1044 - val_mae: 0.2585\n",
      "Epoch 123/250\n",
      "177/177 [==============================] - 12s 66ms/step - loss: 0.0851 - mae: 0.2273 - val_loss: 0.0709 - val_mae: 0.2099\n",
      "Epoch 124/250\n",
      "177/177 [==============================] - 12s 66ms/step - loss: 0.0582 - mae: 0.1838 - val_loss: 0.0438 - val_mae: 0.1720\n",
      "Epoch 125/250\n",
      "177/177 [==============================] - 12s 65ms/step - loss: 0.0425 - mae: 0.1557 - val_loss: 0.0296 - val_mae: 0.1344\n",
      "Epoch 126/250\n",
      "177/177 [==============================] - 12s 65ms/step - loss: 0.0267 - mae: 0.1279 - val_loss: 0.0207 - val_mae: 0.1153\n",
      "Epoch 127/250\n",
      "177/177 [==============================] - 12s 67ms/step - loss: 0.0211 - mae: 0.1114 - val_loss: 0.0185 - val_mae: 0.1078\n",
      "Epoch 128/250\n",
      "177/177 [==============================] - 12s 65ms/step - loss: 0.0176 - mae: 0.1041 - val_loss: 0.0162 - val_mae: 0.1032\n",
      "Epoch 129/250\n",
      "177/177 [==============================] - 12s 65ms/step - loss: 0.0192 - mae: 0.1093 - val_loss: 0.0151 - val_mae: 0.0990\n",
      "Epoch 130/250\n",
      "177/177 [==============================] - 12s 66ms/step - loss: 0.0154 - mae: 0.0961 - val_loss: 0.0159 - val_mae: 0.0989\n",
      "Epoch 131/250\n",
      "177/177 [==============================] - 12s 65ms/step - loss: 0.0145 - mae: 0.0927 - val_loss: 0.0158 - val_mae: 0.0987\n",
      "Epoch 132/250\n",
      "177/177 [==============================] - 12s 65ms/step - loss: 0.0118 - mae: 0.0837 - val_loss: 0.0228 - val_mae: 0.1271\n",
      "Epoch 133/250\n",
      "177/177 [==============================] - 12s 65ms/step - loss: 0.0118 - mae: 0.0860 - val_loss: 0.0119 - val_mae: 0.0865\n",
      "Epoch 134/250\n",
      "177/177 [==============================] - 11s 65ms/step - loss: 0.0123 - mae: 0.0857 - val_loss: 0.0155 - val_mae: 0.0964\n",
      "\n",
      "Now Training (66)\n",
      "{'length': 180, 'layers_num': 2, 'layers_type': 'LSTM', 'units': 40, 'g_filt': 2.0}\n",
      "Epoch 1/250\n",
      "177/177 [==============================] - 16s 70ms/step - loss: 74.9269 - mae: 7.2328 - val_loss: 9.2666 - val_mae: 2.3519\n",
      "Epoch 2/250\n",
      "177/177 [==============================] - 12s 65ms/step - loss: 13.4356 - mae: 2.8240 - val_loss: 3.3807 - val_mae: 1.3908\n",
      "Epoch 3/250\n",
      "177/177 [==============================] - 12s 65ms/step - loss: 6.9044 - mae: 1.9220 - val_loss: 1.6195 - val_mae: 0.9764\n",
      "Epoch 4/250\n",
      "177/177 [==============================] - 12s 65ms/step - loss: 3.0419 - mae: 1.1780 - val_loss: 1.0572 - val_mae: 0.7747\n",
      "Epoch 5/250\n",
      "177/177 [==============================] - 12s 65ms/step - loss: 1.5929 - mae: 0.8371 - val_loss: 0.9014 - val_mae: 0.7679\n",
      "Epoch 6/250\n",
      "177/177 [==============================] - 11s 65ms/step - loss: 1.5886 - mae: 0.8082 - val_loss: 0.4868 - val_mae: 0.5170\n",
      "Epoch 7/250\n",
      "177/177 [==============================] - 11s 65ms/step - loss: 1.0967 - mae: 0.6536 - val_loss: 0.2798 - val_mae: 0.3909\n",
      "Epoch 8/250\n",
      "177/177 [==============================] - 11s 65ms/step - loss: 0.5856 - mae: 0.4986 - val_loss: 0.4126 - val_mae: 0.5217\n",
      "Epoch 9/250\n",
      "177/177 [==============================] - 12s 65ms/step - loss: 0.4110 - mae: 0.4188 - val_loss: 0.1741 - val_mae: 0.3179\n",
      "Epoch 10/250\n",
      "177/177 [==============================] - 12s 65ms/step - loss: 0.3322 - mae: 0.3528 - val_loss: 0.1390 - val_mae: 0.2847\n",
      "Epoch 11/250\n",
      "177/177 [==============================] - 11s 65ms/step - loss: 0.2628 - mae: 0.3151 - val_loss: 0.1180 - val_mae: 0.2550\n",
      "Epoch 12/250\n",
      "177/177 [==============================] - 12s 65ms/step - loss: 0.2345 - mae: 0.3274 - val_loss: 0.1037 - val_mae: 0.2273\n",
      "Epoch 13/250\n",
      "177/177 [==============================] - 12s 65ms/step - loss: 0.1643 - mae: 0.2632 - val_loss: 0.2315 - val_mae: 0.3932\n",
      "Epoch 14/250\n",
      "177/177 [==============================] - 12s 66ms/step - loss: 0.2121 - mae: 0.3072 - val_loss: 0.0899 - val_mae: 0.2105\n",
      "Epoch 15/250\n",
      "177/177 [==============================] - 12s 65ms/step - loss: 0.1472 - mae: 0.2410 - val_loss: 0.0615 - val_mae: 0.1736\n",
      "Epoch 16/250\n",
      "177/177 [==============================] - 12s 65ms/step - loss: 0.0770 - mae: 0.1840 - val_loss: 0.1085 - val_mae: 0.2892\n",
      "Epoch 17/250\n",
      "177/177 [==============================] - 12s 66ms/step - loss: 0.1206 - mae: 0.2439 - val_loss: 0.0417 - val_mae: 0.1460\n",
      "Epoch 18/250\n",
      "177/177 [==============================] - 12s 65ms/step - loss: 0.0775 - mae: 0.1854 - val_loss: 0.0401 - val_mae: 0.1399\n",
      "Epoch 19/250\n",
      "177/177 [==============================] - 12s 66ms/step - loss: 0.0885 - mae: 0.1992 - val_loss: 0.0433 - val_mae: 0.1586\n",
      "Epoch 20/250\n",
      "177/177 [==============================] - 12s 65ms/step - loss: 0.0699 - mae: 0.1865 - val_loss: 0.0802 - val_mae: 0.2391\n",
      "Epoch 21/250\n",
      "177/177 [==============================] - 12s 65ms/step - loss: 0.0909 - mae: 0.2130 - val_loss: 0.0300 - val_mae: 0.1260\n",
      "Epoch 22/250\n",
      "177/177 [==============================] - 11s 65ms/step - loss: 0.0738 - mae: 0.1936 - val_loss: 0.0278 - val_mae: 0.1274\n",
      "Epoch 23/250\n",
      "177/177 [==============================] - 12s 65ms/step - loss: 0.0607 - mae: 0.1816 - val_loss: 0.0661 - val_mae: 0.2147\n",
      "Epoch 24/250\n",
      "177/177 [==============================] - 12s 66ms/step - loss: 0.0648 - mae: 0.1917 - val_loss: 0.0190 - val_mae: 0.0985\n",
      "Epoch 25/250\n",
      "177/177 [==============================] - 12s 66ms/step - loss: 0.0453 - mae: 0.1418 - val_loss: 0.0175 - val_mae: 0.0979\n",
      "Epoch 26/250\n",
      "177/177 [==============================] - 12s 65ms/step - loss: 0.0432 - mae: 0.1385 - val_loss: 0.0401 - val_mae: 0.1637\n",
      "Epoch 27/250\n",
      "177/177 [==============================] - 12s 66ms/step - loss: 0.0368 - mae: 0.1435 - val_loss: 0.1630 - val_mae: 0.3851\n",
      "Epoch 28/250\n",
      "177/177 [==============================] - 12s 65ms/step - loss: 0.0672 - mae: 0.1875 - val_loss: 0.0130 - val_mae: 0.0851\n",
      "Epoch 29/250\n",
      "177/177 [==============================] - 12s 65ms/step - loss: 0.0394 - mae: 0.1404 - val_loss: 0.0514 - val_mae: 0.2001\n",
      "Epoch 30/250\n",
      "177/177 [==============================] - 12s 66ms/step - loss: 0.0464 - mae: 0.1659 - val_loss: 0.0261 - val_mae: 0.1410\n",
      "Epoch 31/250\n",
      "177/177 [==============================] - 12s 65ms/step - loss: 0.0261 - mae: 0.1190 - val_loss: 0.0360 - val_mae: 0.1532\n",
      "Epoch 32/250\n",
      "177/177 [==============================] - 12s 65ms/step - loss: 0.0817 - mae: 0.2128 - val_loss: 0.0156 - val_mae: 0.1022\n",
      "Epoch 33/250\n",
      "177/177 [==============================] - 12s 66ms/step - loss: 0.0589 - mae: 0.1853 - val_loss: 0.0095 - val_mae: 0.0701\n",
      "Epoch 34/250\n",
      "177/177 [==============================] - 12s 65ms/step - loss: 0.0232 - mae: 0.1045 - val_loss: 0.0101 - val_mae: 0.0703\n",
      "Epoch 35/250\n",
      "177/177 [==============================] - 12s 66ms/step - loss: 0.0166 - mae: 0.0918 - val_loss: 0.0078 - val_mae: 0.0643\n",
      "Epoch 36/250\n",
      "177/177 [==============================] - 12s 67ms/step - loss: 0.0344 - mae: 0.1324 - val_loss: 0.0230 - val_mae: 0.1094\n",
      "Epoch 37/250\n",
      "177/177 [==============================] - 12s 66ms/step - loss: 0.0265 - mae: 0.1135 - val_loss: 0.0083 - val_mae: 0.0705\n",
      "Epoch 38/250\n",
      "177/177 [==============================] - 12s 66ms/step - loss: 0.0175 - mae: 0.0944 - val_loss: 0.0091 - val_mae: 0.0706\n",
      "Epoch 39/250\n",
      "177/177 [==============================] - 12s 66ms/step - loss: 0.0220 - mae: 0.1087 - val_loss: 0.0249 - val_mae: 0.1003\n",
      "Epoch 40/250\n",
      "177/177 [==============================] - 12s 66ms/step - loss: 0.0949 - mae: 0.2216 - val_loss: 0.0165 - val_mae: 0.0929\n",
      "Epoch 41/250\n",
      "177/177 [==============================] - 12s 66ms/step - loss: 0.0219 - mae: 0.1132 - val_loss: 0.0105 - val_mae: 0.0795\n",
      "Epoch 42/250\n",
      "177/177 [==============================] - 12s 65ms/step - loss: 0.0135 - mae: 0.0896 - val_loss: 0.0059 - val_mae: 0.0621\n",
      "Epoch 43/250\n",
      "177/177 [==============================] - 12s 66ms/step - loss: 0.0210 - mae: 0.1078 - val_loss: 0.0189 - val_mae: 0.1154\n",
      "Epoch 44/250\n",
      "177/177 [==============================] - 12s 66ms/step - loss: 0.0341 - mae: 0.1471 - val_loss: 0.0074 - val_mae: 0.0686\n",
      "Epoch 45/250\n",
      "177/177 [==============================] - 12s 66ms/step - loss: 0.0113 - mae: 0.0814 - val_loss: 0.0715 - val_mae: 0.2537\n",
      "Epoch 46/250\n",
      "177/177 [==============================] - 12s 66ms/step - loss: 0.0134 - mae: 0.0826 - val_loss: 0.0278 - val_mae: 0.1326\n",
      "Epoch 47/250\n",
      "177/177 [==============================] - 12s 66ms/step - loss: 0.0234 - mae: 0.1229 - val_loss: 0.0194 - val_mae: 0.1139\n",
      "Epoch 48/250\n",
      "177/177 [==============================] - 12s 65ms/step - loss: 0.0434 - mae: 0.1649 - val_loss: 0.0081 - val_mae: 0.0726\n",
      "Epoch 49/250\n",
      "177/177 [==============================] - 12s 66ms/step - loss: 0.0228 - mae: 0.1103 - val_loss: 0.0172 - val_mae: 0.0891\n",
      "Epoch 50/250\n",
      "177/177 [==============================] - 12s 67ms/step - loss: 0.0080 - mae: 0.0666 - val_loss: 0.1368 - val_mae: 0.3550\n",
      "Epoch 51/250\n",
      "177/177 [==============================] - 12s 67ms/step - loss: 0.0440 - mae: 0.1631 - val_loss: 0.0050 - val_mae: 0.0621\n",
      "Epoch 52/250\n",
      "177/177 [==============================] - 12s 66ms/step - loss: 0.0056 - mae: 0.0556 - val_loss: 0.0116 - val_mae: 0.0866\n",
      "Epoch 53/250\n",
      "177/177 [==============================] - 12s 65ms/step - loss: 0.0431 - mae: 0.1487 - val_loss: 0.0412 - val_mae: 0.1924\n",
      "Epoch 54/250\n",
      "177/177 [==============================] - 12s 66ms/step - loss: 0.0270 - mae: 0.1290 - val_loss: 0.0074 - val_mae: 0.0703\n",
      "Epoch 55/250\n",
      "177/177 [==============================] - 12s 65ms/step - loss: 0.0308 - mae: 0.1083 - val_loss: 0.2145 - val_mae: 0.3740\n",
      "Epoch 56/250\n",
      "177/177 [==============================] - 12s 65ms/step - loss: 0.1600 - mae: 0.2911 - val_loss: 0.0312 - val_mae: 0.1621\n",
      "Epoch 57/250\n",
      "177/177 [==============================] - 12s 66ms/step - loss: 0.0251 - mae: 0.1172 - val_loss: 0.0049 - val_mae: 0.0567\n",
      "Epoch 58/250\n",
      "177/177 [==============================] - 12s 66ms/step - loss: 0.0053 - mae: 0.0520 - val_loss: 0.0029 - val_mae: 0.0403\n",
      "Epoch 59/250\n",
      "177/177 [==============================] - 12s 66ms/step - loss: 0.0029 - mae: 0.0404 - val_loss: 0.0036 - val_mae: 0.0462\n",
      "Epoch 60/250\n",
      "177/177 [==============================] - 12s 66ms/step - loss: 0.0045 - mae: 0.0490 - val_loss: 0.0067 - val_mae: 0.0640\n",
      "Epoch 61/250\n",
      "177/177 [==============================] - 12s 65ms/step - loss: 0.0058 - mae: 0.0564 - val_loss: 0.0035 - val_mae: 0.0447\n",
      "Epoch 62/250\n",
      "177/177 [==============================] - 12s 65ms/step - loss: 0.0057 - mae: 0.0559 - val_loss: 0.0047 - val_mae: 0.0504\n",
      "Epoch 63/250\n",
      "177/177 [==============================] - 12s 66ms/step - loss: 0.0072 - mae: 0.0599 - val_loss: 0.1251 - val_mae: 0.3055\n",
      "Epoch 64/250\n",
      "177/177 [==============================] - 12s 66ms/step - loss: 0.0712 - mae: 0.2073 - val_loss: 0.0752 - val_mae: 0.2559\n",
      "Epoch 65/250\n",
      "177/177 [==============================] - 12s 65ms/step - loss: 0.0484 - mae: 0.1570 - val_loss: 0.0922 - val_mae: 0.2931\n",
      "Epoch 66/250\n",
      "177/177 [==============================] - 12s 66ms/step - loss: 0.0261 - mae: 0.1215 - val_loss: 0.0032 - val_mae: 0.0448\n",
      "Epoch 67/250\n",
      "177/177 [==============================] - 12s 66ms/step - loss: 0.0062 - mae: 0.0560 - val_loss: 0.0512 - val_mae: 0.2090\n",
      "Epoch 68/250\n",
      "177/177 [==============================] - 12s 65ms/step - loss: 0.0185 - mae: 0.0963 - val_loss: 0.0036 - val_mae: 0.0406\n",
      "Epoch 69/250\n",
      "177/177 [==============================] - 12s 65ms/step - loss: 0.0088 - mae: 0.0765 - val_loss: 0.0030 - val_mae: 0.0416\n",
      "Epoch 70/250\n",
      "177/177 [==============================] - 12s 65ms/step - loss: 0.0046 - mae: 0.0492 - val_loss: 0.0039 - val_mae: 0.0487\n",
      "Epoch 71/250\n",
      "177/177 [==============================] - 12s 66ms/step - loss: 0.0118 - mae: 0.0796 - val_loss: 0.0150 - val_mae: 0.0984\n",
      "Epoch 72/250\n",
      "177/177 [==============================] - 12s 65ms/step - loss: 0.0223 - mae: 0.1154 - val_loss: 0.0076 - val_mae: 0.0741\n",
      "Epoch 73/250\n",
      "177/177 [==============================] - 12s 65ms/step - loss: 0.0093 - mae: 0.0747 - val_loss: 0.0095 - val_mae: 0.0885\n",
      "Epoch 74/250\n",
      "177/177 [==============================] - 12s 65ms/step - loss: 0.0335 - mae: 0.1227 - val_loss: 0.0063 - val_mae: 0.0665\n",
      "Epoch 75/250\n",
      "177/177 [==============================] - 12s 65ms/step - loss: 0.0327 - mae: 0.1235 - val_loss: 0.0025 - val_mae: 0.0364\n",
      "Epoch 76/250\n",
      "177/177 [==============================] - 11s 65ms/step - loss: 0.0051 - mae: 0.0524 - val_loss: 0.0021 - val_mae: 0.0328\n",
      "Epoch 77/250\n",
      "177/177 [==============================] - 12s 65ms/step - loss: 0.0170 - mae: 0.0933 - val_loss: 0.1307 - val_mae: 0.3345\n",
      "Epoch 78/250\n",
      "177/177 [==============================] - 12s 65ms/step - loss: 0.0292 - mae: 0.1057 - val_loss: 0.0029 - val_mae: 0.0399\n",
      "Epoch 79/250\n",
      "177/177 [==============================] - 12s 65ms/step - loss: 0.0024 - mae: 0.0361 - val_loss: 0.0019 - val_mae: 0.0334\n",
      "Epoch 80/250\n",
      "177/177 [==============================] - 12s 66ms/step - loss: 0.0019 - mae: 0.0309 - val_loss: 0.0017 - val_mae: 0.0292\n",
      "Epoch 81/250\n",
      "177/177 [==============================] - 12s 66ms/step - loss: 0.0074 - mae: 0.0568 - val_loss: 0.0023 - val_mae: 0.0384\n",
      "Epoch 82/250\n",
      "177/177 [==============================] - 12s 66ms/step - loss: 0.0043 - mae: 0.0497 - val_loss: 0.0024 - val_mae: 0.0387\n",
      "Epoch 83/250\n",
      "177/177 [==============================] - 12s 68ms/step - loss: 0.0052 - mae: 0.0511 - val_loss: 0.0128 - val_mae: 0.0878\n",
      "Epoch 84/250\n",
      "177/177 [==============================] - 12s 67ms/step - loss: 0.0480 - mae: 0.1473 - val_loss: 0.0165 - val_mae: 0.1154\n",
      "Epoch 85/250\n",
      "177/177 [==============================] - 12s 67ms/step - loss: 0.0147 - mae: 0.0892 - val_loss: 0.0021 - val_mae: 0.0376\n",
      "Epoch 86/250\n",
      "177/177 [==============================] - 12s 66ms/step - loss: 0.0024 - mae: 0.0370 - val_loss: 0.0083 - val_mae: 0.0658\n",
      "Epoch 87/250\n",
      "177/177 [==============================] - 12s 67ms/step - loss: 0.0284 - mae: 0.1232 - val_loss: 0.0020 - val_mae: 0.0322\n",
      "Epoch 88/250\n",
      "177/177 [==============================] - 12s 66ms/step - loss: 0.0064 - mae: 0.0613 - val_loss: 0.0013 - val_mae: 0.0261\n",
      "Epoch 89/250\n",
      "177/177 [==============================] - 12s 65ms/step - loss: 0.0037 - mae: 0.0419 - val_loss: 0.0031 - val_mae: 0.0469\n",
      "Epoch 90/250\n",
      "177/177 [==============================] - 12s 66ms/step - loss: 0.0042 - mae: 0.0465 - val_loss: 0.0022 - val_mae: 0.0369\n",
      "Epoch 91/250\n",
      "177/177 [==============================] - 12s 66ms/step - loss: 0.0175 - mae: 0.0906 - val_loss: 0.0076 - val_mae: 0.0705\n",
      "Epoch 92/250\n",
      "177/177 [==============================] - 12s 67ms/step - loss: 0.0111 - mae: 0.0836 - val_loss: 0.0126 - val_mae: 0.1020\n",
      "Epoch 93/250\n",
      "177/177 [==============================] - 12s 66ms/step - loss: 0.0053 - mae: 0.0527 - val_loss: 0.0031 - val_mae: 0.0383\n",
      "Epoch 94/250\n",
      "177/177 [==============================] - 12s 66ms/step - loss: 0.0112 - mae: 0.0774 - val_loss: 0.0066 - val_mae: 0.0633\n",
      "Epoch 95/250\n",
      "177/177 [==============================] - 12s 67ms/step - loss: 0.0124 - mae: 0.0844 - val_loss: 0.0052 - val_mae: 0.0557\n",
      "Epoch 96/250\n",
      "177/177 [==============================] - 12s 67ms/step - loss: 0.0054 - mae: 0.0591 - val_loss: 0.0010 - val_mae: 0.0243\n",
      "Epoch 97/250\n",
      "177/177 [==============================] - 12s 67ms/step - loss: 0.0040 - mae: 0.0408 - val_loss: 0.0020 - val_mae: 0.0308\n",
      "Epoch 98/250\n",
      "177/177 [==============================] - 12s 66ms/step - loss: 0.0031 - mae: 0.0400 - val_loss: 0.0015 - val_mae: 0.0298\n",
      "Epoch 99/250\n",
      "177/177 [==============================] - 12s 66ms/step - loss: 0.0021 - mae: 0.0336 - val_loss: 0.0155 - val_mae: 0.1144\n",
      "Epoch 100/250\n",
      "177/177 [==============================] - 12s 66ms/step - loss: 0.0273 - mae: 0.1141 - val_loss: 0.0170 - val_mae: 0.1234\n",
      "Epoch 101/250\n",
      "177/177 [==============================] - 12s 66ms/step - loss: 0.0143 - mae: 0.0894 - val_loss: 0.0023 - val_mae: 0.0340\n",
      "Epoch 102/250\n",
      "177/177 [==============================] - 12s 67ms/step - loss: 0.0058 - mae: 0.0585 - val_loss: 0.0013 - val_mae: 0.0261\n",
      "Epoch 103/250\n",
      "177/177 [==============================] - 12s 66ms/step - loss: 0.0049 - mae: 0.0525 - val_loss: 0.0226 - val_mae: 0.1350\n",
      "Epoch 104/250\n",
      "177/177 [==============================] - 12s 66ms/step - loss: 0.0058 - mae: 0.0552 - val_loss: 0.0022 - val_mae: 0.0369\n",
      "Epoch 105/250\n",
      "177/177 [==============================] - 12s 66ms/step - loss: 0.0052 - mae: 0.0518 - val_loss: 0.0072 - val_mae: 0.0638\n",
      "Epoch 106/250\n",
      "177/177 [==============================] - 12s 66ms/step - loss: 0.0235 - mae: 0.1179 - val_loss: 0.0024 - val_mae: 0.0383\n",
      "Epoch 107/250\n",
      "177/177 [==============================] - 12s 66ms/step - loss: 0.0119 - mae: 0.0714 - val_loss: 0.0059 - val_mae: 0.0672\n",
      "Epoch 108/250\n",
      "177/177 [==============================] - 12s 66ms/step - loss: 0.0227 - mae: 0.1131 - val_loss: 0.0274 - val_mae: 0.1580\n",
      "Epoch 109/250\n",
      "177/177 [==============================] - 12s 66ms/step - loss: 0.0077 - mae: 0.0671 - val_loss: 8.5691e-04 - val_mae: 0.0215\n",
      "Epoch 110/250\n",
      "177/177 [==============================] - 12s 65ms/step - loss: 0.0073 - mae: 0.0602 - val_loss: 0.0041 - val_mae: 0.0538\n",
      "Epoch 111/250\n",
      "177/177 [==============================] - 12s 66ms/step - loss: 0.0092 - mae: 0.0671 - val_loss: 0.0030 - val_mae: 0.0488\n",
      "Epoch 112/250\n",
      "177/177 [==============================] - 12s 66ms/step - loss: 0.0021 - mae: 0.0351 - val_loss: 0.0023 - val_mae: 0.0401\n",
      "Epoch 113/250\n",
      "177/177 [==============================] - 12s 66ms/step - loss: 0.0167 - mae: 0.0888 - val_loss: 0.0029 - val_mae: 0.0441\n",
      "Epoch 114/250\n",
      "177/177 [==============================] - 12s 65ms/step - loss: 0.0058 - mae: 0.0592 - val_loss: 0.0028 - val_mae: 0.0455\n",
      "Epoch 115/250\n",
      "177/177 [==============================] - 12s 67ms/step - loss: 0.0031 - mae: 0.0417 - val_loss: 0.0019 - val_mae: 0.0320\n",
      "Epoch 116/250\n",
      "177/177 [==============================] - 12s 66ms/step - loss: 0.0021 - mae: 0.0354 - val_loss: 0.0030 - val_mae: 0.0456\n",
      "Epoch 117/250\n",
      "177/177 [==============================] - 12s 67ms/step - loss: 0.0016 - mae: 0.0301 - val_loss: 0.0017 - val_mae: 0.0321\n",
      "Epoch 118/250\n",
      "177/177 [==============================] - 12s 66ms/step - loss: 0.0036 - mae: 0.0435 - val_loss: 0.0010 - val_mae: 0.0236\n",
      "Epoch 119/250\n",
      "177/177 [==============================] - 12s 67ms/step - loss: 0.0038 - mae: 0.0409 - val_loss: 0.0315 - val_mae: 0.1734\n",
      "Epoch 120/250\n",
      "177/177 [==============================] - 12s 67ms/step - loss: 0.2291 - mae: 0.3119 - val_loss: 0.0153 - val_mae: 0.1059\n",
      "Epoch 121/250\n",
      "177/177 [==============================] - 12s 66ms/step - loss: 0.0303 - mae: 0.1223 - val_loss: 0.0026 - val_mae: 0.0395\n",
      "Epoch 122/250\n",
      "177/177 [==============================] - 12s 66ms/step - loss: 0.0054 - mae: 0.0529 - val_loss: 0.0023 - val_mae: 0.0364\n",
      "Epoch 123/250\n",
      "177/177 [==============================] - 12s 66ms/step - loss: 0.0059 - mae: 0.0579 - val_loss: 0.0024 - val_mae: 0.0385\n",
      "Epoch 124/250\n",
      "177/177 [==============================] - 12s 66ms/step - loss: 0.0077 - mae: 0.0645 - val_loss: 0.0025 - val_mae: 0.0377\n",
      "Epoch 125/250\n",
      "177/177 [==============================] - 12s 66ms/step - loss: 0.0041 - mae: 0.0470 - val_loss: 0.0019 - val_mae: 0.0318\n",
      "Epoch 126/250\n",
      "177/177 [==============================] - 12s 66ms/step - loss: 0.0030 - mae: 0.0389 - val_loss: 0.0045 - val_mae: 0.0569\n",
      "Epoch 127/250\n",
      "177/177 [==============================] - 12s 66ms/step - loss: 0.0361 - mae: 0.1391 - val_loss: 0.0073 - val_mae: 0.0632\n",
      "Epoch 128/250\n",
      "177/177 [==============================] - 12s 66ms/step - loss: 0.0240 - mae: 0.1265 - val_loss: 0.0018 - val_mae: 0.0324\n",
      "Epoch 129/250\n",
      "177/177 [==============================] - 12s 67ms/step - loss: 0.0064 - mae: 0.0559 - val_loss: 0.0032 - val_mae: 0.0391\n",
      "\n",
      "Now Training (67)\n",
      "{'length': 180, 'layers_num': 2, 'layers_type': 'LSTM', 'units': 40, 'g_filt': 2.5}\n",
      "Epoch 1/250\n",
      "177/177 [==============================] - 16s 72ms/step - loss: 71.9258 - mae: 7.0068 - val_loss: 7.5421 - val_mae: 2.1745\n",
      "Epoch 2/250\n",
      "177/177 [==============================] - 12s 67ms/step - loss: 12.8238 - mae: 2.7362 - val_loss: 2.4372 - val_mae: 1.2015\n",
      "Epoch 3/250\n",
      "177/177 [==============================] - 12s 67ms/step - loss: 5.3312 - mae: 1.6025 - val_loss: 0.9601 - val_mae: 0.7606\n",
      "Epoch 4/250\n",
      "177/177 [==============================] - 12s 67ms/step - loss: 2.3481 - mae: 1.0039 - val_loss: 0.5636 - val_mae: 0.5800\n",
      "Epoch 5/250\n",
      "177/177 [==============================] - 12s 68ms/step - loss: 1.4561 - mae: 0.8078 - val_loss: 0.3987 - val_mae: 0.4720\n",
      "Epoch 6/250\n",
      "177/177 [==============================] - 12s 68ms/step - loss: 0.8028 - mae: 0.5815 - val_loss: 0.3275 - val_mae: 0.4098\n",
      "Epoch 7/250\n",
      "177/177 [==============================] - 12s 67ms/step - loss: 0.5537 - mae: 0.4714 - val_loss: 0.1746 - val_mae: 0.3069\n",
      "Epoch 8/250\n",
      "177/177 [==============================] - 12s 68ms/step - loss: 0.3390 - mae: 0.3347 - val_loss: 0.1741 - val_mae: 0.3139\n",
      "Epoch 9/250\n",
      "177/177 [==============================] - 12s 67ms/step - loss: 0.3168 - mae: 0.3705 - val_loss: 0.1099 - val_mae: 0.2411\n",
      "Epoch 10/250\n",
      "177/177 [==============================] - 12s 68ms/step - loss: 0.2235 - mae: 0.2842 - val_loss: 0.1052 - val_mae: 0.2535\n",
      "Epoch 11/250\n",
      "177/177 [==============================] - 12s 68ms/step - loss: 0.1699 - mae: 0.2652 - val_loss: 0.0741 - val_mae: 0.1957\n",
      "Epoch 12/250\n",
      "177/177 [==============================] - 12s 67ms/step - loss: 0.1277 - mae: 0.2422 - val_loss: 0.0583 - val_mae: 0.1755\n",
      "Epoch 13/250\n",
      "177/177 [==============================] - 12s 67ms/step - loss: 0.1192 - mae: 0.2272 - val_loss: 0.0732 - val_mae: 0.2182\n",
      "Epoch 14/250\n",
      "177/177 [==============================] - 12s 67ms/step - loss: 0.1161 - mae: 0.2488 - val_loss: 0.1225 - val_mae: 0.3018\n",
      "Epoch 15/250\n",
      "177/177 [==============================] - 12s 68ms/step - loss: 0.1435 - mae: 0.2602 - val_loss: 0.0697 - val_mae: 0.2093\n",
      "Epoch 16/250\n",
      "177/177 [==============================] - 12s 68ms/step - loss: 0.0989 - mae: 0.2232 - val_loss: 0.0473 - val_mae: 0.1783\n",
      "Epoch 17/250\n",
      "177/177 [==============================] - 12s 68ms/step - loss: 0.0871 - mae: 0.2030 - val_loss: 0.0662 - val_mae: 0.2087\n",
      "Epoch 18/250\n",
      "177/177 [==============================] - 12s 68ms/step - loss: 0.0613 - mae: 0.1727 - val_loss: 0.0270 - val_mae: 0.1187\n",
      "Epoch 19/250\n",
      "177/177 [==============================] - 12s 68ms/step - loss: 0.2369 - mae: 0.3637 - val_loss: 0.0904 - val_mae: 0.2457\n",
      "Epoch 20/250\n",
      "177/177 [==============================] - 12s 68ms/step - loss: 0.1113 - mae: 0.2408 - val_loss: 0.0307 - val_mae: 0.1411\n",
      "Epoch 21/250\n",
      "177/177 [==============================] - 12s 69ms/step - loss: 0.0472 - mae: 0.1485 - val_loss: 0.0205 - val_mae: 0.1074\n",
      "Epoch 22/250\n",
      "177/177 [==============================] - 12s 68ms/step - loss: 0.0271 - mae: 0.1065 - val_loss: 0.0171 - val_mae: 0.0989\n",
      "Epoch 23/250\n",
      "177/177 [==============================] - 12s 69ms/step - loss: 0.0304 - mae: 0.1170 - val_loss: 0.0268 - val_mae: 0.1351\n",
      "Epoch 24/250\n",
      "177/177 [==============================] - 12s 68ms/step - loss: 0.0194 - mae: 0.0967 - val_loss: 0.0177 - val_mae: 0.1070\n",
      "Epoch 25/250\n",
      "177/177 [==============================] - 12s 68ms/step - loss: 0.0459 - mae: 0.1513 - val_loss: 0.0138 - val_mae: 0.0888\n",
      "Epoch 26/250\n",
      "177/177 [==============================] - 12s 68ms/step - loss: 0.0202 - mae: 0.0973 - val_loss: 0.0323 - val_mae: 0.1555\n",
      "Epoch 27/250\n",
      "177/177 [==============================] - 12s 68ms/step - loss: 0.0267 - mae: 0.1181 - val_loss: 0.0277 - val_mae: 0.1288\n",
      "Epoch 28/250\n",
      "177/177 [==============================] - 12s 69ms/step - loss: 0.0224 - mae: 0.1126 - val_loss: 0.0184 - val_mae: 0.1064\n",
      "Epoch 29/250\n",
      "177/177 [==============================] - 12s 68ms/step - loss: 0.0326 - mae: 0.1421 - val_loss: 0.0567 - val_mae: 0.2160\n",
      "Epoch 30/250\n",
      "177/177 [==============================] - 12s 68ms/step - loss: 0.0363 - mae: 0.1490 - val_loss: 0.1290 - val_mae: 0.3198\n",
      "Epoch 31/250\n",
      "177/177 [==============================] - 12s 69ms/step - loss: 0.0752 - mae: 0.1960 - val_loss: 0.0458 - val_mae: 0.1922\n",
      "Epoch 32/250\n",
      "177/177 [==============================] - 12s 68ms/step - loss: 0.0414 - mae: 0.1627 - val_loss: 0.0148 - val_mae: 0.0953\n",
      "Epoch 33/250\n",
      "177/177 [==============================] - 12s 69ms/step - loss: 0.0247 - mae: 0.1129 - val_loss: 0.0353 - val_mae: 0.1292\n",
      "Epoch 34/250\n",
      "177/177 [==============================] - 12s 68ms/step - loss: 0.0254 - mae: 0.1172 - val_loss: 0.0214 - val_mae: 0.1240\n",
      "Epoch 35/250\n",
      "177/177 [==============================] - 12s 69ms/step - loss: 0.0486 - mae: 0.1644 - val_loss: 0.1913 - val_mae: 0.3285\n",
      "Epoch 36/250\n",
      "177/177 [==============================] - 12s 68ms/step - loss: 0.2277 - mae: 0.3860 - val_loss: 0.0126 - val_mae: 0.0793\n",
      "Epoch 37/250\n",
      "177/177 [==============================] - 12s 68ms/step - loss: 0.0151 - mae: 0.0836 - val_loss: 0.0132 - val_mae: 0.0909\n",
      "Epoch 38/250\n",
      "177/177 [==============================] - 12s 68ms/step - loss: 0.0103 - mae: 0.0742 - val_loss: 0.0072 - val_mae: 0.0648\n",
      "Epoch 39/250\n",
      "177/177 [==============================] - 12s 68ms/step - loss: 0.0078 - mae: 0.0623 - val_loss: 0.0086 - val_mae: 0.0660\n",
      "Epoch 40/250\n",
      "177/177 [==============================] - 12s 68ms/step - loss: 0.0096 - mae: 0.0720 - val_loss: 0.0089 - val_mae: 0.0774\n",
      "Epoch 41/250\n",
      "177/177 [==============================] - 12s 68ms/step - loss: 0.0109 - mae: 0.0767 - val_loss: 0.0252 - val_mae: 0.1460\n",
      "Epoch 42/250\n",
      "177/177 [==============================] - 12s 68ms/step - loss: 0.0511 - mae: 0.1733 - val_loss: 0.0166 - val_mae: 0.1043\n",
      "Epoch 43/250\n",
      "177/177 [==============================] - 12s 69ms/step - loss: 0.0847 - mae: 0.2196 - val_loss: 0.0088 - val_mae: 0.0690\n",
      "Epoch 44/250\n",
      "177/177 [==============================] - 12s 68ms/step - loss: 0.0070 - mae: 0.0602 - val_loss: 0.0182 - val_mae: 0.1039\n",
      "Epoch 45/250\n",
      "177/177 [==============================] - 12s 68ms/step - loss: 0.0108 - mae: 0.0775 - val_loss: 0.0088 - val_mae: 0.0828\n",
      "Epoch 46/250\n",
      "177/177 [==============================] - 12s 69ms/step - loss: 0.0151 - mae: 0.0816 - val_loss: 0.0095 - val_mae: 0.0859\n",
      "Epoch 47/250\n",
      "177/177 [==============================] - 12s 68ms/step - loss: 0.0071 - mae: 0.0597 - val_loss: 0.0037 - val_mae: 0.0440\n",
      "Epoch 48/250\n",
      "177/177 [==============================] - 12s 69ms/step - loss: 0.0722 - mae: 0.2119 - val_loss: 0.0124 - val_mae: 0.0850\n",
      "Epoch 49/250\n",
      "177/177 [==============================] - 12s 69ms/step - loss: 0.0438 - mae: 0.1527 - val_loss: 0.0051 - val_mae: 0.0536\n",
      "Epoch 50/250\n",
      "177/177 [==============================] - 12s 69ms/step - loss: 0.0137 - mae: 0.0811 - val_loss: 0.0075 - val_mae: 0.0741\n",
      "Epoch 51/250\n",
      "177/177 [==============================] - 12s 69ms/step - loss: 0.0074 - mae: 0.0632 - val_loss: 0.0086 - val_mae: 0.0793\n",
      "Epoch 52/250\n",
      "177/177 [==============================] - 12s 69ms/step - loss: 0.0044 - mae: 0.0477 - val_loss: 0.0046 - val_mae: 0.0535\n",
      "Epoch 53/250\n",
      "177/177 [==============================] - 12s 69ms/step - loss: 0.0177 - mae: 0.1012 - val_loss: 0.0035 - val_mae: 0.0452\n",
      "Epoch 54/250\n",
      "177/177 [==============================] - 12s 69ms/step - loss: 0.0050 - mae: 0.0533 - val_loss: 0.0055 - val_mae: 0.0544\n",
      "Epoch 55/250\n",
      "177/177 [==============================] - 12s 69ms/step - loss: 0.0197 - mae: 0.0982 - val_loss: 0.2493 - val_mae: 0.4876\n",
      "Epoch 56/250\n",
      "177/177 [==============================] - 12s 69ms/step - loss: 0.0336 - mae: 0.1276 - val_loss: 0.0079 - val_mae: 0.0569\n",
      "Epoch 57/250\n",
      "177/177 [==============================] - 12s 68ms/step - loss: 0.0215 - mae: 0.1138 - val_loss: 0.0025 - val_mae: 0.0354\n",
      "Epoch 58/250\n",
      "177/177 [==============================] - 12s 69ms/step - loss: 0.0093 - mae: 0.0705 - val_loss: 0.0047 - val_mae: 0.0573\n",
      "Epoch 59/250\n",
      "177/177 [==============================] - 12s 69ms/step - loss: 0.0180 - mae: 0.1007 - val_loss: 0.0127 - val_mae: 0.0839\n",
      "Epoch 60/250\n",
      "177/177 [==============================] - 12s 69ms/step - loss: 0.0066 - mae: 0.0637 - val_loss: 0.0038 - val_mae: 0.0492\n",
      "Epoch 61/250\n",
      "177/177 [==============================] - 12s 69ms/step - loss: 0.0090 - mae: 0.0733 - val_loss: 0.0031 - val_mae: 0.0440\n",
      "Epoch 62/250\n",
      "177/177 [==============================] - 12s 69ms/step - loss: 0.0198 - mae: 0.0906 - val_loss: 0.0084 - val_mae: 0.0779\n",
      "Epoch 63/250\n",
      "177/177 [==============================] - 12s 69ms/step - loss: 0.0190 - mae: 0.1081 - val_loss: 0.0077 - val_mae: 0.0724\n",
      "Epoch 64/250\n",
      "177/177 [==============================] - 12s 69ms/step - loss: 0.0060 - mae: 0.0584 - val_loss: 0.0029 - val_mae: 0.0428\n",
      "Epoch 65/250\n",
      "177/177 [==============================] - 12s 68ms/step - loss: 0.0077 - mae: 0.0677 - val_loss: 0.0054 - val_mae: 0.0637\n",
      "Epoch 66/250\n",
      "177/177 [==============================] - 12s 69ms/step - loss: 0.0059 - mae: 0.0587 - val_loss: 0.0081 - val_mae: 0.0668\n",
      "Epoch 67/250\n",
      "177/177 [==============================] - 12s 68ms/step - loss: 0.0851 - mae: 0.2214 - val_loss: 0.0065 - val_mae: 0.0721\n",
      "Epoch 68/250\n",
      "177/177 [==============================] - 12s 69ms/step - loss: 0.0106 - mae: 0.0803 - val_loss: 0.0137 - val_mae: 0.1098\n",
      "Epoch 69/250\n",
      "177/177 [==============================] - 12s 69ms/step - loss: 0.0089 - mae: 0.0742 - val_loss: 0.0158 - val_mae: 0.1140\n",
      "Epoch 70/250\n",
      "177/177 [==============================] - 12s 69ms/step - loss: 0.0129 - mae: 0.0892 - val_loss: 0.0118 - val_mae: 0.0933\n",
      "Epoch 71/250\n",
      "177/177 [==============================] - 12s 69ms/step - loss: 0.0073 - mae: 0.0665 - val_loss: 0.0035 - val_mae: 0.0415\n",
      "Epoch 72/250\n",
      "177/177 [==============================] - 12s 69ms/step - loss: 0.0031 - mae: 0.0406 - val_loss: 0.0106 - val_mae: 0.0586\n",
      "Epoch 73/250\n",
      "177/177 [==============================] - 12s 68ms/step - loss: 0.0506 - mae: 0.1661 - val_loss: 0.0060 - val_mae: 0.0643\n",
      "Epoch 74/250\n",
      "177/177 [==============================] - 12s 69ms/step - loss: 0.0060 - mae: 0.0579 - val_loss: 0.0077 - val_mae: 0.0774\n",
      "Epoch 75/250\n",
      "177/177 [==============================] - 12s 69ms/step - loss: 0.0101 - mae: 0.0744 - val_loss: 0.0038 - val_mae: 0.0540\n",
      "Epoch 76/250\n",
      "177/177 [==============================] - 12s 68ms/step - loss: 0.0032 - mae: 0.0440 - val_loss: 0.0089 - val_mae: 0.0737\n",
      "Epoch 77/250\n",
      "177/177 [==============================] - 12s 68ms/step - loss: 0.0040 - mae: 0.0438 - val_loss: 0.0199 - val_mae: 0.1248\n",
      "\n",
      "Now Training (68)\n",
      "{'length': 180, 'layers_num': 2, 'layers_type': 'LSTM', 'units': 80, 'g_filt': 1.0}\n",
      "Epoch 1/250\n",
      "177/177 [==============================] - 19s 88ms/step - loss: 60.1935 - mae: 6.0849 - val_loss: 6.4653 - val_mae: 2.0297\n",
      "Epoch 2/250\n",
      "177/177 [==============================] - 15s 84ms/step - loss: 8.7484 - mae: 2.3155 - val_loss: 2.9522 - val_mae: 1.3516\n",
      "Epoch 3/250\n",
      "177/177 [==============================] - 15s 83ms/step - loss: 2.9161 - mae: 1.3210 - val_loss: 1.7066 - val_mae: 1.0143\n",
      "Epoch 4/250\n",
      "177/177 [==============================] - 15s 84ms/step - loss: 1.4257 - mae: 0.9048 - val_loss: 1.0112 - val_mae: 0.7785\n",
      "Epoch 5/250\n",
      "177/177 [==============================] - 15s 83ms/step - loss: 1.2397 - mae: 0.8113 - val_loss: 0.7166 - val_mae: 0.6501\n",
      "Epoch 6/250\n",
      "177/177 [==============================] - 15s 82ms/step - loss: 0.7628 - mae: 0.6473 - val_loss: 0.5328 - val_mae: 0.5579\n",
      "Epoch 7/250\n",
      "177/177 [==============================] - 15s 83ms/step - loss: 0.4396 - mae: 0.4919 - val_loss: 0.3888 - val_mae: 0.4725\n",
      "Epoch 8/250\n",
      "177/177 [==============================] - 15s 83ms/step - loss: 0.3946 - mae: 0.4650 - val_loss: 0.4808 - val_mae: 0.5391\n",
      "Epoch 9/250\n",
      "177/177 [==============================] - 15s 83ms/step - loss: 0.3979 - mae: 0.4725 - val_loss: 0.3994 - val_mae: 0.5196\n",
      "Epoch 10/250\n",
      "177/177 [==============================] - 15s 84ms/step - loss: 0.3418 - mae: 0.4468 - val_loss: 0.3146 - val_mae: 0.4304\n",
      "Epoch 11/250\n",
      "177/177 [==============================] - 15s 83ms/step - loss: 0.2792 - mae: 0.4058 - val_loss: 0.2774 - val_mae: 0.4062\n",
      "Epoch 12/250\n",
      "177/177 [==============================] - 15s 83ms/step - loss: 0.3490 - mae: 0.4460 - val_loss: 0.2595 - val_mae: 0.3873\n",
      "Epoch 13/250\n",
      "177/177 [==============================] - 15s 83ms/step - loss: 0.2418 - mae: 0.3838 - val_loss: 0.2567 - val_mae: 0.3893\n",
      "Epoch 14/250\n",
      "177/177 [==============================] - 15s 84ms/step - loss: 0.2405 - mae: 0.3726 - val_loss: 0.2452 - val_mae: 0.3830\n",
      "Epoch 15/250\n",
      "177/177 [==============================] - 15s 83ms/step - loss: 0.2153 - mae: 0.3645 - val_loss: 0.2473 - val_mae: 0.3824\n",
      "Epoch 16/250\n",
      "177/177 [==============================] - 15s 83ms/step - loss: 0.3171 - mae: 0.4517 - val_loss: 0.2584 - val_mae: 0.3972\n",
      "Epoch 17/250\n",
      "177/177 [==============================] - 15s 82ms/step - loss: 0.2424 - mae: 0.3858 - val_loss: 0.2284 - val_mae: 0.3760\n",
      "Epoch 18/250\n",
      "177/177 [==============================] - 15s 84ms/step - loss: 0.2192 - mae: 0.3661 - val_loss: 0.2097 - val_mae: 0.3584\n",
      "Epoch 19/250\n",
      "177/177 [==============================] - 15s 83ms/step - loss: 0.1991 - mae: 0.3528 - val_loss: 0.2704 - val_mae: 0.4150\n",
      "Epoch 20/250\n",
      "177/177 [==============================] - 15s 83ms/step - loss: 0.1930 - mae: 0.3481 - val_loss: 0.1965 - val_mae: 0.3500\n",
      "Epoch 21/250\n",
      "177/177 [==============================] - 15s 83ms/step - loss: 0.2171 - mae: 0.3703 - val_loss: 0.6315 - val_mae: 0.6836\n",
      "Epoch 22/250\n",
      "177/177 [==============================] - 15s 84ms/step - loss: 0.3522 - mae: 0.4740 - val_loss: 0.2009 - val_mae: 0.3549\n",
      "Epoch 23/250\n",
      "177/177 [==============================] - 15s 84ms/step - loss: 0.2308 - mae: 0.3814 - val_loss: 0.4124 - val_mae: 0.5390\n",
      "Epoch 24/250\n",
      "177/177 [==============================] - 15s 84ms/step - loss: 0.2302 - mae: 0.3735 - val_loss: 0.5055 - val_mae: 0.5704\n",
      "Epoch 25/250\n",
      "177/177 [==============================] - 15s 84ms/step - loss: 0.2528 - mae: 0.4033 - val_loss: 0.2125 - val_mae: 0.3629\n",
      "Epoch 26/250\n",
      "177/177 [==============================] - 15s 84ms/step - loss: 0.2128 - mae: 0.3618 - val_loss: 0.3208 - val_mae: 0.4650\n",
      "Epoch 27/250\n",
      "177/177 [==============================] - 15s 83ms/step - loss: 0.2482 - mae: 0.3945 - val_loss: 0.2416 - val_mae: 0.3873\n",
      "Epoch 28/250\n",
      "177/177 [==============================] - 15s 84ms/step - loss: 0.2067 - mae: 0.3640 - val_loss: 0.1858 - val_mae: 0.3391\n",
      "Epoch 29/250\n",
      "177/177 [==============================] - 15s 83ms/step - loss: 0.1790 - mae: 0.3406 - val_loss: 0.2423 - val_mae: 0.3896\n",
      "Epoch 30/250\n",
      "177/177 [==============================] - 15s 83ms/step - loss: 0.1947 - mae: 0.3522 - val_loss: 0.2622 - val_mae: 0.4066\n",
      "Epoch 31/250\n",
      "177/177 [==============================] - 15s 84ms/step - loss: 0.3275 - mae: 0.4526 - val_loss: 0.2612 - val_mae: 0.4145\n",
      "Epoch 32/250\n",
      "177/177 [==============================] - 15s 83ms/step - loss: 0.2537 - mae: 0.3978 - val_loss: 0.2563 - val_mae: 0.4025\n",
      "Epoch 33/250\n",
      "177/177 [==============================] - 15s 83ms/step - loss: 0.2787 - mae: 0.4151 - val_loss: 0.1997 - val_mae: 0.3483\n",
      "Epoch 34/250\n",
      "177/177 [==============================] - 15s 83ms/step - loss: 0.1858 - mae: 0.3392 - val_loss: 0.1920 - val_mae: 0.3463\n",
      "Epoch 35/250\n",
      "177/177 [==============================] - 15s 83ms/step - loss: 0.1722 - mae: 0.3322 - val_loss: 0.2026 - val_mae: 0.3572\n",
      "Epoch 36/250\n",
      "177/177 [==============================] - 15s 83ms/step - loss: 0.1719 - mae: 0.3319 - val_loss: 0.2106 - val_mae: 0.3693\n",
      "Epoch 37/250\n",
      "177/177 [==============================] - 15s 84ms/step - loss: 0.2143 - mae: 0.3713 - val_loss: 0.1855 - val_mae: 0.3326\n",
      "Epoch 38/250\n",
      "177/177 [==============================] - 15s 84ms/step - loss: 0.2255 - mae: 0.3740 - val_loss: 0.2437 - val_mae: 0.3889\n",
      "Epoch 39/250\n",
      "177/177 [==============================] - 15s 84ms/step - loss: 0.1932 - mae: 0.3496 - val_loss: 0.3151 - val_mae: 0.4384\n",
      "Epoch 40/250\n",
      "177/177 [==============================] - 15s 83ms/step - loss: 0.2155 - mae: 0.3663 - val_loss: 0.1770 - val_mae: 0.3320\n",
      "Epoch 41/250\n",
      "177/177 [==============================] - 15s 83ms/step - loss: 0.1888 - mae: 0.3451 - val_loss: 0.2383 - val_mae: 0.3804\n",
      "Epoch 42/250\n",
      "177/177 [==============================] - 15s 83ms/step - loss: 0.1816 - mae: 0.3367 - val_loss: 0.1964 - val_mae: 0.3462\n",
      "Epoch 43/250\n",
      "177/177 [==============================] - 15s 84ms/step - loss: 0.2186 - mae: 0.3734 - val_loss: 0.1971 - val_mae: 0.3522\n",
      "Epoch 44/250\n",
      "177/177 [==============================] - 15s 83ms/step - loss: 0.1988 - mae: 0.3578 - val_loss: 0.1888 - val_mae: 0.3351\n",
      "Epoch 45/250\n",
      "177/177 [==============================] - 15s 83ms/step - loss: 0.1817 - mae: 0.3421 - val_loss: 0.3681 - val_mae: 0.4844\n",
      "Epoch 46/250\n",
      "177/177 [==============================] - 15s 84ms/step - loss: 0.2024 - mae: 0.3599 - val_loss: 0.2262 - val_mae: 0.3759\n",
      "Epoch 47/250\n",
      "177/177 [==============================] - 15s 83ms/step - loss: 0.1966 - mae: 0.3473 - val_loss: 0.2505 - val_mae: 0.4002\n",
      "Epoch 48/250\n",
      "177/177 [==============================] - 15s 83ms/step - loss: 0.2011 - mae: 0.3550 - val_loss: 0.1920 - val_mae: 0.3445\n",
      "Epoch 49/250\n",
      "177/177 [==============================] - 15s 83ms/step - loss: 0.1744 - mae: 0.3333 - val_loss: 0.2055 - val_mae: 0.3549\n",
      "Epoch 50/250\n",
      "177/177 [==============================] - 15s 83ms/step - loss: 0.1943 - mae: 0.3509 - val_loss: 0.1733 - val_mae: 0.3265\n",
      "Epoch 51/250\n",
      "177/177 [==============================] - 15s 84ms/step - loss: 0.1674 - mae: 0.3274 - val_loss: 0.1780 - val_mae: 0.3304\n",
      "Epoch 52/250\n",
      "177/177 [==============================] - 15s 83ms/step - loss: 0.1832 - mae: 0.3415 - val_loss: 0.2876 - val_mae: 0.4332\n",
      "Epoch 53/250\n",
      "177/177 [==============================] - 15s 84ms/step - loss: 0.1938 - mae: 0.3519 - val_loss: 0.2048 - val_mae: 0.3624\n",
      "Epoch 54/250\n",
      "177/177 [==============================] - 15s 83ms/step - loss: 0.1817 - mae: 0.3384 - val_loss: 0.2479 - val_mae: 0.3856\n",
      "Epoch 55/250\n",
      "177/177 [==============================] - 15s 84ms/step - loss: 0.2600 - mae: 0.4044 - val_loss: 0.2581 - val_mae: 0.4077\n",
      "Epoch 56/250\n",
      "177/177 [==============================] - 15s 84ms/step - loss: 0.1802 - mae: 0.3401 - val_loss: 0.2296 - val_mae: 0.3803\n",
      "Epoch 57/250\n",
      "177/177 [==============================] - 15s 84ms/step - loss: 0.1846 - mae: 0.3431 - val_loss: 0.1962 - val_mae: 0.3479\n",
      "Epoch 58/250\n",
      "177/177 [==============================] - 15s 83ms/step - loss: 0.1729 - mae: 0.3309 - val_loss: 0.2005 - val_mae: 0.3540\n",
      "Epoch 59/250\n",
      "177/177 [==============================] - 15s 83ms/step - loss: 0.1827 - mae: 0.3420 - val_loss: 0.1787 - val_mae: 0.3281\n",
      "Epoch 60/250\n",
      "177/177 [==============================] - 15s 84ms/step - loss: 0.1597 - mae: 0.3198 - val_loss: 0.2209 - val_mae: 0.3736\n",
      "Epoch 61/250\n",
      "177/177 [==============================] - 15s 83ms/step - loss: 0.1726 - mae: 0.3286 - val_loss: 0.2966 - val_mae: 0.4427\n",
      "Epoch 62/250\n",
      "177/177 [==============================] - 15s 84ms/step - loss: 0.3157 - mae: 0.4379 - val_loss: 0.1876 - val_mae: 0.3354\n",
      "Epoch 63/250\n",
      "177/177 [==============================] - 15s 84ms/step - loss: 0.2165 - mae: 0.3680 - val_loss: 0.1849 - val_mae: 0.3350\n",
      "Epoch 64/250\n",
      "177/177 [==============================] - 15s 84ms/step - loss: 0.1620 - mae: 0.3194 - val_loss: 0.2060 - val_mae: 0.3559\n",
      "Epoch 65/250\n",
      "177/177 [==============================] - 15s 84ms/step - loss: 0.2159 - mae: 0.3686 - val_loss: 0.2124 - val_mae: 0.3624\n",
      "Epoch 66/250\n",
      "177/177 [==============================] - 15s 83ms/step - loss: 0.1702 - mae: 0.3284 - val_loss: 0.1892 - val_mae: 0.3446\n",
      "Epoch 67/250\n",
      "177/177 [==============================] - 15s 83ms/step - loss: 0.1689 - mae: 0.3297 - val_loss: 0.1881 - val_mae: 0.3381\n",
      "Epoch 68/250\n",
      "177/177 [==============================] - 15s 84ms/step - loss: 0.1760 - mae: 0.3330 - val_loss: 0.1777 - val_mae: 0.3277\n",
      "Epoch 69/250\n",
      "177/177 [==============================] - 15s 83ms/step - loss: 0.1502 - mae: 0.3124 - val_loss: 0.2152 - val_mae: 0.3649\n",
      "Epoch 70/250\n",
      "177/177 [==============================] - 15s 83ms/step - loss: 0.1655 - mae: 0.3238 - val_loss: 0.1932 - val_mae: 0.3376\n",
      "\n",
      "Now Training (69)\n",
      "{'length': 180, 'layers_num': 2, 'layers_type': 'LSTM', 'units': 80, 'g_filt': 1.5}\n",
      "Epoch 1/250\n",
      "177/177 [==============================] - 19s 89ms/step - loss: 37.8057 - mae: 4.8462 - val_loss: 3.9311 - val_mae: 1.6221\n",
      "Epoch 2/250\n",
      "177/177 [==============================] - 15s 84ms/step - loss: 3.6593 - mae: 1.4655 - val_loss: 1.6998 - val_mae: 1.0292\n",
      "Epoch 3/250\n",
      "177/177 [==============================] - 15s 83ms/step - loss: 1.6192 - mae: 0.9363 - val_loss: 0.9324 - val_mae: 0.7186\n",
      "Epoch 4/250\n",
      "177/177 [==============================] - 15s 83ms/step - loss: 0.9664 - mae: 0.7110 - val_loss: 0.7207 - val_mae: 0.6186\n",
      "Epoch 5/250\n",
      "177/177 [==============================] - 15s 84ms/step - loss: 0.6657 - mae: 0.5970 - val_loss: 0.4278 - val_mae: 0.4837\n",
      "Epoch 6/250\n",
      "177/177 [==============================] - 15s 84ms/step - loss: 0.4123 - mae: 0.4653 - val_loss: 0.3598 - val_mae: 0.4540\n",
      "Epoch 7/250\n",
      "177/177 [==============================] - 15s 84ms/step - loss: 0.3969 - mae: 0.4178 - val_loss: 0.2345 - val_mae: 0.3502\n",
      "Epoch 8/250\n",
      "177/177 [==============================] - 15s 84ms/step - loss: 0.2490 - mae: 0.3579 - val_loss: 0.2441 - val_mae: 0.3901\n",
      "Epoch 9/250\n",
      "177/177 [==============================] - 15s 83ms/step - loss: 0.2455 - mae: 0.3614 - val_loss: 0.1504 - val_mae: 0.2844\n",
      "Epoch 10/250\n",
      "177/177 [==============================] - 15s 83ms/step - loss: 0.1655 - mae: 0.2922 - val_loss: 0.1496 - val_mae: 0.2880\n",
      "Epoch 11/250\n",
      "177/177 [==============================] - 15s 83ms/step - loss: 0.1905 - mae: 0.3169 - val_loss: 0.1069 - val_mae: 0.2343\n",
      "Epoch 12/250\n",
      "177/177 [==============================] - 15s 83ms/step - loss: 0.1052 - mae: 0.2218 - val_loss: 0.0812 - val_mae: 0.1983\n",
      "Epoch 13/250\n",
      "177/177 [==============================] - 15s 83ms/step - loss: 0.1197 - mae: 0.2472 - val_loss: 0.0729 - val_mae: 0.1949\n",
      "Epoch 14/250\n",
      "177/177 [==============================] - 15s 83ms/step - loss: 0.1102 - mae: 0.2368 - val_loss: 0.1491 - val_mae: 0.3360\n",
      "Epoch 15/250\n",
      "177/177 [==============================] - 15s 83ms/step - loss: 0.1537 - mae: 0.2993 - val_loss: 0.0549 - val_mae: 0.1702\n",
      "Epoch 16/250\n",
      "177/177 [==============================] - 15s 83ms/step - loss: 0.0545 - mae: 0.1666 - val_loss: 0.0568 - val_mae: 0.1675\n",
      "Epoch 17/250\n",
      "177/177 [==============================] - 15s 84ms/step - loss: 0.0845 - mae: 0.2156 - val_loss: 0.1315 - val_mae: 0.2725\n",
      "Epoch 18/250\n",
      "177/177 [==============================] - 15s 83ms/step - loss: 0.0717 - mae: 0.2040 - val_loss: 0.0374 - val_mae: 0.1411\n",
      "Epoch 19/250\n",
      "177/177 [==============================] - 15s 83ms/step - loss: 0.0411 - mae: 0.1479 - val_loss: 0.0324 - val_mae: 0.1255\n",
      "Epoch 20/250\n",
      "177/177 [==============================] - 15s 83ms/step - loss: 0.0358 - mae: 0.1386 - val_loss: 0.0295 - val_mae: 0.1330\n",
      "Epoch 21/250\n",
      "177/177 [==============================] - 15s 84ms/step - loss: 0.0381 - mae: 0.1478 - val_loss: 0.0711 - val_mae: 0.2386\n",
      "Epoch 22/250\n",
      "177/177 [==============================] - 15s 84ms/step - loss: 0.0300 - mae: 0.1284 - val_loss: 0.1231 - val_mae: 0.2964\n",
      "Epoch 23/250\n",
      "177/177 [==============================] - 15s 83ms/step - loss: 0.0577 - mae: 0.1838 - val_loss: 0.0462 - val_mae: 0.1806\n",
      "Epoch 24/250\n",
      "177/177 [==============================] - 15s 83ms/step - loss: 0.0340 - mae: 0.1405 - val_loss: 0.0160 - val_mae: 0.0891\n",
      "Epoch 25/250\n",
      "177/177 [==============================] - 15s 84ms/step - loss: 0.0298 - mae: 0.1240 - val_loss: 0.9663 - val_mae: 0.8450\n",
      "Epoch 26/250\n",
      "177/177 [==============================] - 15s 83ms/step - loss: 0.4171 - mae: 0.4883 - val_loss: 0.1921 - val_mae: 0.3283\n",
      "Epoch 27/250\n",
      "177/177 [==============================] - 15s 84ms/step - loss: 0.1285 - mae: 0.2835 - val_loss: 0.0501 - val_mae: 0.1720\n",
      "Epoch 28/250\n",
      "177/177 [==============================] - 15s 84ms/step - loss: 0.0722 - mae: 0.2111 - val_loss: 0.0358 - val_mae: 0.1486\n",
      "Epoch 29/250\n",
      "177/177 [==============================] - 15s 83ms/step - loss: 0.0496 - mae: 0.1750 - val_loss: 0.0330 - val_mae: 0.1380\n",
      "Epoch 30/250\n",
      "177/177 [==============================] - 15s 84ms/step - loss: 0.0414 - mae: 0.1561 - val_loss: 0.0422 - val_mae: 0.1744\n",
      "Epoch 31/250\n",
      "177/177 [==============================] - 15s 83ms/step - loss: 0.0331 - mae: 0.1397 - val_loss: 0.0272 - val_mae: 0.1344\n",
      "Epoch 32/250\n",
      "177/177 [==============================] - 15s 83ms/step - loss: 0.0519 - mae: 0.1651 - val_loss: 0.0220 - val_mae: 0.1150\n",
      "Epoch 33/250\n",
      "177/177 [==============================] - 15s 83ms/step - loss: 0.0410 - mae: 0.1532 - val_loss: 0.0864 - val_mae: 0.2628\n",
      "Epoch 34/250\n",
      "177/177 [==============================] - 15s 84ms/step - loss: 0.0294 - mae: 0.1334 - val_loss: 0.0165 - val_mae: 0.0985\n",
      "Epoch 35/250\n",
      "177/177 [==============================] - 15s 83ms/step - loss: 0.0213 - mae: 0.1145 - val_loss: 0.0147 - val_mae: 0.0983\n",
      "Epoch 36/250\n",
      "177/177 [==============================] - 15s 83ms/step - loss: 0.0214 - mae: 0.1118 - val_loss: 0.0608 - val_mae: 0.2155\n",
      "Epoch 37/250\n",
      "177/177 [==============================] - 15s 84ms/step - loss: 0.0363 - mae: 0.1514 - val_loss: 0.1185 - val_mae: 0.3063\n",
      "Epoch 38/250\n",
      "177/177 [==============================] - 15s 84ms/step - loss: 0.0508 - mae: 0.1728 - val_loss: 0.0785 - val_mae: 0.2171\n",
      "Epoch 39/250\n",
      "177/177 [==============================] - 15s 84ms/step - loss: 0.0452 - mae: 0.1677 - val_loss: 0.0265 - val_mae: 0.1347\n",
      "Epoch 40/250\n",
      "177/177 [==============================] - 15s 84ms/step - loss: 0.0201 - mae: 0.1046 - val_loss: 0.0442 - val_mae: 0.1893\n",
      "Epoch 41/250\n",
      "177/177 [==============================] - 15s 83ms/step - loss: 0.0211 - mae: 0.1088 - val_loss: 1.5361 - val_mae: 1.2263\n",
      "Epoch 42/250\n",
      "177/177 [==============================] - 15s 83ms/step - loss: 0.5453 - mae: 0.5338 - val_loss: 0.0144 - val_mae: 0.0997\n",
      "Epoch 43/250\n",
      "177/177 [==============================] - 15s 84ms/step - loss: 0.0128 - mae: 0.0857 - val_loss: 0.0093 - val_mae: 0.0736\n",
      "Epoch 44/250\n",
      "177/177 [==============================] - 15s 84ms/step - loss: 0.0128 - mae: 0.0842 - val_loss: 0.0474 - val_mae: 0.1915\n",
      "Epoch 45/250\n",
      "177/177 [==============================] - 15s 83ms/step - loss: 0.0298 - mae: 0.1330 - val_loss: 0.0078 - val_mae: 0.0689\n",
      "Epoch 46/250\n",
      "177/177 [==============================] - 15s 83ms/step - loss: 0.0391 - mae: 0.1463 - val_loss: 0.0616 - val_mae: 0.2296\n",
      "Epoch 47/250\n",
      "177/177 [==============================] - 15s 84ms/step - loss: 0.0155 - mae: 0.0970 - val_loss: 0.0119 - val_mae: 0.0882\n",
      "Epoch 48/250\n",
      "177/177 [==============================] - 15s 84ms/step - loss: 0.0178 - mae: 0.1029 - val_loss: 0.0072 - val_mae: 0.0670\n",
      "Epoch 49/250\n",
      "177/177 [==============================] - 15s 83ms/step - loss: 0.0095 - mae: 0.0750 - val_loss: 0.0128 - val_mae: 0.0855\n",
      "Epoch 50/250\n",
      "177/177 [==============================] - 15s 84ms/step - loss: 0.0192 - mae: 0.1069 - val_loss: 0.0164 - val_mae: 0.0928\n",
      "Epoch 51/250\n",
      "177/177 [==============================] - 15s 83ms/step - loss: 0.0178 - mae: 0.1031 - val_loss: 0.0208 - val_mae: 0.1229\n",
      "Epoch 52/250\n",
      "177/177 [==============================] - 15s 84ms/step - loss: 0.0247 - mae: 0.1256 - val_loss: 0.0205 - val_mae: 0.1234\n",
      "Epoch 53/250\n",
      "177/177 [==============================] - 15s 84ms/step - loss: 0.0239 - mae: 0.1213 - val_loss: 0.0318 - val_mae: 0.1489\n",
      "Epoch 54/250\n",
      "177/177 [==============================] - 15s 84ms/step - loss: 0.0378 - mae: 0.1495 - val_loss: 0.0229 - val_mae: 0.1307\n",
      "Epoch 55/250\n",
      "177/177 [==============================] - 15s 83ms/step - loss: 0.0318 - mae: 0.1370 - val_loss: 0.0258 - val_mae: 0.1340\n",
      "Epoch 56/250\n",
      "177/177 [==============================] - 15s 84ms/step - loss: 0.0311 - mae: 0.1396 - val_loss: 0.0101 - val_mae: 0.0835\n",
      "Epoch 57/250\n",
      "177/177 [==============================] - 15s 84ms/step - loss: 0.0298 - mae: 0.1244 - val_loss: 0.0207 - val_mae: 0.1156\n",
      "Epoch 58/250\n",
      "177/177 [==============================] - 15s 84ms/step - loss: 0.0400 - mae: 0.1375 - val_loss: 0.0079 - val_mae: 0.0703\n",
      "Epoch 59/250\n",
      "177/177 [==============================] - 15s 84ms/step - loss: 0.0157 - mae: 0.0897 - val_loss: 0.0189 - val_mae: 0.1061\n",
      "Epoch 60/250\n",
      "177/177 [==============================] - 15s 83ms/step - loss: 0.0148 - mae: 0.0904 - val_loss: 0.0074 - val_mae: 0.0666\n",
      "Epoch 61/250\n",
      "177/177 [==============================] - 15s 84ms/step - loss: 0.0157 - mae: 0.0915 - val_loss: 0.0079 - val_mae: 0.0691\n",
      "Epoch 62/250\n",
      "177/177 [==============================] - 15s 83ms/step - loss: 0.0094 - mae: 0.0767 - val_loss: 0.0058 - val_mae: 0.0603\n",
      "Epoch 63/250\n",
      "177/177 [==============================] - 15s 84ms/step - loss: 0.0078 - mae: 0.0684 - val_loss: 0.0112 - val_mae: 0.0854\n",
      "Epoch 64/250\n",
      "177/177 [==============================] - 15s 84ms/step - loss: 0.0280 - mae: 0.1331 - val_loss: 0.0450 - val_mae: 0.1627\n",
      "Epoch 65/250\n",
      "177/177 [==============================] - 15s 83ms/step - loss: 0.0246 - mae: 0.1250 - val_loss: 0.0145 - val_mae: 0.1007\n",
      "Epoch 66/250\n",
      "177/177 [==============================] - 15s 84ms/step - loss: 0.0143 - mae: 0.0920 - val_loss: 0.0118 - val_mae: 0.0822\n",
      "Epoch 67/250\n",
      "177/177 [==============================] - 15s 83ms/step - loss: 0.0248 - mae: 0.1132 - val_loss: 0.0323 - val_mae: 0.1587\n",
      "Epoch 68/250\n",
      "177/177 [==============================] - 15s 83ms/step - loss: 0.0159 - mae: 0.0963 - val_loss: 0.0331 - val_mae: 0.1448\n",
      "Epoch 69/250\n",
      "177/177 [==============================] - 15s 83ms/step - loss: 0.0522 - mae: 0.1602 - val_loss: 0.0063 - val_mae: 0.0628\n",
      "Epoch 70/250\n",
      "177/177 [==============================] - 15s 83ms/step - loss: 0.0097 - mae: 0.0768 - val_loss: 0.0076 - val_mae: 0.0709\n",
      "Epoch 71/250\n",
      "177/177 [==============================] - 15s 83ms/step - loss: 0.0122 - mae: 0.0838 - val_loss: 0.0068 - val_mae: 0.0640\n",
      "Epoch 72/250\n",
      "177/177 [==============================] - 15s 84ms/step - loss: 0.0074 - mae: 0.0671 - val_loss: 0.0089 - val_mae: 0.0719\n",
      "Epoch 73/250\n",
      "177/177 [==============================] - 15s 83ms/step - loss: 0.0199 - mae: 0.0954 - val_loss: 0.1546 - val_mae: 0.3504\n",
      "Epoch 74/250\n",
      "177/177 [==============================] - 15s 83ms/step - loss: 0.0500 - mae: 0.1683 - val_loss: 0.0125 - val_mae: 0.0937\n",
      "Epoch 75/250\n",
      "177/177 [==============================] - 15s 83ms/step - loss: 0.0108 - mae: 0.0769 - val_loss: 0.0145 - val_mae: 0.0925\n",
      "Epoch 76/250\n",
      "177/177 [==============================] - 15s 83ms/step - loss: 0.0178 - mae: 0.1073 - val_loss: 0.0326 - val_mae: 0.1555\n",
      "Epoch 77/250\n",
      "177/177 [==============================] - 15s 83ms/step - loss: 0.0158 - mae: 0.0991 - val_loss: 0.0048 - val_mae: 0.0545\n",
      "Epoch 78/250\n",
      "177/177 [==============================] - 15s 84ms/step - loss: 0.0117 - mae: 0.0793 - val_loss: 0.0124 - val_mae: 0.0926\n",
      "Epoch 79/250\n",
      "177/177 [==============================] - 15s 83ms/step - loss: 0.0203 - mae: 0.0995 - val_loss: 0.0294 - val_mae: 0.1315\n",
      "Epoch 80/250\n",
      "177/177 [==============================] - 15s 84ms/step - loss: 0.0564 - mae: 0.1840 - val_loss: 0.0501 - val_mae: 0.2074\n",
      "Epoch 81/250\n",
      "177/177 [==============================] - 15s 83ms/step - loss: 0.0142 - mae: 0.0919 - val_loss: 0.0060 - val_mae: 0.0613\n",
      "Epoch 82/250\n",
      "177/177 [==============================] - 15s 83ms/step - loss: 0.0088 - mae: 0.0720 - val_loss: 0.0064 - val_mae: 0.0623\n",
      "Epoch 83/250\n",
      "177/177 [==============================] - 15s 83ms/step - loss: 0.0195 - mae: 0.1088 - val_loss: 0.0106 - val_mae: 0.0875\n",
      "Epoch 84/250\n",
      "177/177 [==============================] - 15s 84ms/step - loss: 0.0245 - mae: 0.1199 - val_loss: 0.0113 - val_mae: 0.0780\n",
      "Epoch 85/250\n",
      "177/177 [==============================] - 15s 84ms/step - loss: 0.0212 - mae: 0.1099 - val_loss: 0.0074 - val_mae: 0.0648\n",
      "Epoch 86/250\n",
      "177/177 [==============================] - 15s 83ms/step - loss: 0.0151 - mae: 0.0925 - val_loss: 0.0050 - val_mae: 0.0545\n",
      "Epoch 87/250\n",
      "177/177 [==============================] - 15s 83ms/step - loss: 0.0055 - mae: 0.0564 - val_loss: 0.0062 - val_mae: 0.0644\n",
      "Epoch 88/250\n",
      "177/177 [==============================] - 15s 84ms/step - loss: 0.0137 - mae: 0.0899 - val_loss: 0.0333 - val_mae: 0.1601\n",
      "Epoch 89/250\n",
      "177/177 [==============================] - 15s 83ms/step - loss: 0.0262 - mae: 0.1234 - val_loss: 0.0306 - val_mae: 0.1593\n",
      "Epoch 90/250\n",
      "177/177 [==============================] - 15s 83ms/step - loss: 0.2796 - mae: 0.1771 - val_loss: 8.1010 - val_mae: 2.0881\n",
      "Epoch 91/250\n",
      "177/177 [==============================] - 15s 82ms/step - loss: 8.4524 - mae: 2.2610 - val_loss: 1.5448 - val_mae: 0.9960\n",
      "Epoch 92/250\n",
      "177/177 [==============================] - 15s 84ms/step - loss: 1.4078 - mae: 0.9160 - val_loss: 0.9722 - val_mae: 0.7449\n",
      "Epoch 93/250\n",
      "177/177 [==============================] - 15s 84ms/step - loss: 0.7802 - mae: 0.6811 - val_loss: 0.7183 - val_mae: 0.6660\n",
      "Epoch 94/250\n",
      "177/177 [==============================] - 15s 84ms/step - loss: 0.5420 - mae: 0.5556 - val_loss: 0.5521 - val_mae: 0.5609\n",
      "Epoch 95/250\n",
      "177/177 [==============================] - 15s 83ms/step - loss: 0.5003 - mae: 0.5404 - val_loss: 0.5028 - val_mae: 0.5373\n",
      "Epoch 96/250\n",
      "177/177 [==============================] - 15s 84ms/step - loss: 0.3996 - mae: 0.4883 - val_loss: 0.3526 - val_mae: 0.4457\n",
      "Epoch 97/250\n",
      "177/177 [==============================] - 15s 83ms/step - loss: 0.2719 - mae: 0.3936 - val_loss: 0.2178 - val_mae: 0.3511\n",
      "\n",
      "Now Training (70)\n",
      "{'length': 180, 'layers_num': 2, 'layers_type': 'LSTM', 'units': 80, 'g_filt': 2.0}\n",
      "Epoch 1/250\n",
      "177/177 [==============================] - 19s 90ms/step - loss: 32.3228 - mae: 4.3108 - val_loss: 3.3363 - val_mae: 1.5126\n",
      "Epoch 2/250\n",
      "177/177 [==============================] - 15s 84ms/step - loss: 3.5003 - mae: 1.4352 - val_loss: 2.8315 - val_mae: 1.4387\n",
      "Epoch 3/250\n",
      "177/177 [==============================] - 15s 84ms/step - loss: 1.9102 - mae: 1.0336 - val_loss: 0.8280 - val_mae: 0.6837\n",
      "Epoch 4/250\n",
      "177/177 [==============================] - 15s 84ms/step - loss: 0.9794 - mae: 0.7262 - val_loss: 0.7521 - val_mae: 0.6613\n",
      "Epoch 5/250\n",
      "177/177 [==============================] - 15s 83ms/step - loss: 0.4742 - mae: 0.5011 - val_loss: 0.3485 - val_mae: 0.4418\n",
      "Epoch 6/250\n",
      "177/177 [==============================] - 15s 84ms/step - loss: 0.3546 - mae: 0.4073 - val_loss: 0.2613 - val_mae: 0.3785\n",
      "Epoch 7/250\n",
      "177/177 [==============================] - 15s 84ms/step - loss: 0.3341 - mae: 0.3989 - val_loss: 0.2416 - val_mae: 0.3877\n",
      "Epoch 8/250\n",
      "177/177 [==============================] - 15s 82ms/step - loss: 0.2813 - mae: 0.3720 - val_loss: 0.1274 - val_mae: 0.2542\n",
      "Epoch 9/250\n",
      "177/177 [==============================] - 15s 83ms/step - loss: 0.1668 - mae: 0.2817 - val_loss: 0.1908 - val_mae: 0.3665\n",
      "Epoch 10/250\n",
      "177/177 [==============================] - 15s 83ms/step - loss: 0.2055 - mae: 0.3416 - val_loss: 0.1106 - val_mae: 0.2336\n",
      "Epoch 11/250\n",
      "177/177 [==============================] - 15s 83ms/step - loss: 0.1088 - mae: 0.2410 - val_loss: 0.2053 - val_mae: 0.3976\n",
      "Epoch 12/250\n",
      "177/177 [==============================] - 15s 83ms/step - loss: 0.1665 - mae: 0.3074 - val_loss: 0.1179 - val_mae: 0.2782\n",
      "Epoch 13/250\n",
      "177/177 [==============================] - 15s 84ms/step - loss: 0.0795 - mae: 0.2116 - val_loss: 0.0810 - val_mae: 0.2273\n",
      "Epoch 14/250\n",
      "177/177 [==============================] - 15s 83ms/step - loss: 0.0766 - mae: 0.1951 - val_loss: 0.0949 - val_mae: 0.2340\n",
      "Epoch 15/250\n",
      "177/177 [==============================] - 15s 83ms/step - loss: 0.0501 - mae: 0.1634 - val_loss: 0.0601 - val_mae: 0.1977\n",
      "Epoch 16/250\n",
      "177/177 [==============================] - 15s 83ms/step - loss: 0.0511 - mae: 0.1709 - val_loss: 0.1337 - val_mae: 0.3342\n",
      "Epoch 17/250\n",
      "177/177 [==============================] - 15s 83ms/step - loss: 0.0496 - mae: 0.1622 - val_loss: 0.0998 - val_mae: 0.2820\n",
      "Epoch 18/250\n",
      "177/177 [==============================] - 15s 83ms/step - loss: 0.0713 - mae: 0.2038 - val_loss: 0.0243 - val_mae: 0.1127\n",
      "Epoch 19/250\n",
      "177/177 [==============================] - 15s 84ms/step - loss: 0.0485 - mae: 0.1694 - val_loss: 0.0192 - val_mae: 0.1005\n",
      "Epoch 20/250\n",
      "177/177 [==============================] - 15s 83ms/step - loss: 0.0378 - mae: 0.1392 - val_loss: 0.0229 - val_mae: 0.1158\n",
      "Epoch 21/250\n",
      "177/177 [==============================] - 15s 83ms/step - loss: 0.1235 - mae: 0.2662 - val_loss: 0.0636 - val_mae: 0.2288\n",
      "Epoch 22/250\n",
      "177/177 [==============================] - 15s 83ms/step - loss: 0.0573 - mae: 0.1860 - val_loss: 0.0179 - val_mae: 0.0957\n",
      "Epoch 23/250\n",
      "177/177 [==============================] - 15s 84ms/step - loss: 0.0280 - mae: 0.1220 - val_loss: 0.0198 - val_mae: 0.1142\n",
      "Epoch 24/250\n",
      "177/177 [==============================] - 15s 83ms/step - loss: 0.0878 - mae: 0.2373 - val_loss: 0.0495 - val_mae: 0.2012\n",
      "Epoch 25/250\n",
      "177/177 [==============================] - 15s 83ms/step - loss: 0.0522 - mae: 0.1809 - val_loss: 0.0270 - val_mae: 0.1325\n",
      "Epoch 26/250\n",
      "177/177 [==============================] - 15s 84ms/step - loss: 0.0289 - mae: 0.1308 - val_loss: 0.0127 - val_mae: 0.0828\n",
      "Epoch 27/250\n",
      "177/177 [==============================] - 15s 84ms/step - loss: 0.0224 - mae: 0.1124 - val_loss: 0.0129 - val_mae: 0.0909\n",
      "Epoch 28/250\n",
      "177/177 [==============================] - 15s 84ms/step - loss: 0.0172 - mae: 0.0976 - val_loss: 0.0308 - val_mae: 0.1461\n",
      "Epoch 29/250\n",
      "177/177 [==============================] - 15s 84ms/step - loss: 0.0746 - mae: 0.2092 - val_loss: 0.0680 - val_mae: 0.2322\n",
      "Epoch 30/250\n",
      "177/177 [==============================] - 15s 83ms/step - loss: 0.0361 - mae: 0.1351 - val_loss: 0.0129 - val_mae: 0.0833\n",
      "Epoch 31/250\n",
      "177/177 [==============================] - 15s 83ms/step - loss: 0.0323 - mae: 0.1335 - val_loss: 0.0093 - val_mae: 0.0750\n",
      "Epoch 32/250\n",
      "177/177 [==============================] - 15s 83ms/step - loss: 0.0209 - mae: 0.1062 - val_loss: 0.0099 - val_mae: 0.0705\n",
      "Epoch 33/250\n",
      "177/177 [==============================] - 15s 84ms/step - loss: 0.0158 - mae: 0.0939 - val_loss: 0.0212 - val_mae: 0.1201\n",
      "Epoch 34/250\n",
      "177/177 [==============================] - 15s 83ms/step - loss: 0.0566 - mae: 0.1888 - val_loss: 0.0177 - val_mae: 0.0990\n",
      "Epoch 35/250\n",
      "177/177 [==============================] - 15s 83ms/step - loss: 0.0267 - mae: 0.1236 - val_loss: 0.0167 - val_mae: 0.1106\n",
      "Epoch 36/250\n",
      "177/177 [==============================] - 15s 84ms/step - loss: 0.0133 - mae: 0.0849 - val_loss: 0.2621 - val_mae: 0.4663\n",
      "Epoch 37/250\n",
      "177/177 [==============================] - 15s 84ms/step - loss: 0.3195 - mae: 0.4290 - val_loss: 0.0176 - val_mae: 0.0936\n",
      "Epoch 38/250\n",
      "177/177 [==============================] - 15s 84ms/step - loss: 0.0235 - mae: 0.1135 - val_loss: 0.0097 - val_mae: 0.0840\n",
      "Epoch 39/250\n",
      "177/177 [==============================] - 15s 84ms/step - loss: 0.0062 - mae: 0.0596 - val_loss: 0.0073 - val_mae: 0.0633\n",
      "Epoch 40/250\n",
      "177/177 [==============================] - 15s 84ms/step - loss: 0.0091 - mae: 0.0716 - val_loss: 0.0048 - val_mae: 0.0535\n",
      "Epoch 41/250\n",
      "177/177 [==============================] - 15s 83ms/step - loss: 0.0053 - mae: 0.0543 - val_loss: 0.0030 - val_mae: 0.0397\n",
      "Epoch 42/250\n",
      "177/177 [==============================] - 15s 83ms/step - loss: 0.0342 - mae: 0.1176 - val_loss: 0.0030 - val_mae: 0.0409\n",
      "Epoch 43/250\n",
      "177/177 [==============================] - 15s 83ms/step - loss: 0.0169 - mae: 0.0937 - val_loss: 0.3457 - val_mae: 0.5088\n",
      "Epoch 44/250\n",
      "177/177 [==============================] - 15s 84ms/step - loss: 4.4995 - mae: 1.4840 - val_loss: 0.6989 - val_mae: 0.6508\n",
      "Epoch 45/250\n",
      "177/177 [==============================] - 15s 83ms/step - loss: 0.9379 - mae: 0.7539 - val_loss: 0.4165 - val_mae: 0.5076\n",
      "Epoch 46/250\n",
      "177/177 [==============================] - 15s 83ms/step - loss: 0.3003 - mae: 0.4157 - val_loss: 0.2803 - val_mae: 0.4086\n",
      "Epoch 47/250\n",
      "177/177 [==============================] - 15s 84ms/step - loss: 0.2407 - mae: 0.3776 - val_loss: 0.1528 - val_mae: 0.2928\n",
      "Epoch 48/250\n",
      "177/177 [==============================] - 15s 83ms/step - loss: 0.1544 - mae: 0.2898 - val_loss: 0.2173 - val_mae: 0.4010\n",
      "Epoch 49/250\n",
      "177/177 [==============================] - 15s 84ms/step - loss: 0.1522 - mae: 0.3022 - val_loss: 0.0857 - val_mae: 0.2248\n",
      "Epoch 50/250\n",
      "177/177 [==============================] - 15s 83ms/step - loss: 0.0901 - mae: 0.2252 - val_loss: 0.0734 - val_mae: 0.2141\n",
      "Epoch 51/250\n",
      "177/177 [==============================] - 15s 83ms/step - loss: 0.0665 - mae: 0.1927 - val_loss: 0.0742 - val_mae: 0.2004\n",
      "Epoch 52/250\n",
      "177/177 [==============================] - 15s 84ms/step - loss: 0.0504 - mae: 0.1669 - val_loss: 0.0521 - val_mae: 0.1678\n",
      "Epoch 53/250\n",
      "177/177 [==============================] - 15s 83ms/step - loss: 0.0403 - mae: 0.1511 - val_loss: 0.0535 - val_mae: 0.1768\n",
      "Epoch 54/250\n",
      "177/177 [==============================] - 15s 83ms/step - loss: 0.0445 - mae: 0.1574 - val_loss: 0.0493 - val_mae: 0.1718\n",
      "Epoch 55/250\n",
      "177/177 [==============================] - 15s 84ms/step - loss: 0.0340 - mae: 0.1394 - val_loss: 0.0431 - val_mae: 0.1703\n",
      "Epoch 56/250\n",
      "177/177 [==============================] - 15s 84ms/step - loss: 0.0366 - mae: 0.1418 - val_loss: 0.0296 - val_mae: 0.1271\n",
      "Epoch 57/250\n",
      "177/177 [==============================] - 15s 83ms/step - loss: 0.0264 - mae: 0.1204 - val_loss: 0.0280 - val_mae: 0.1282\n",
      "Epoch 58/250\n",
      "177/177 [==============================] - 15s 84ms/step - loss: 0.0337 - mae: 0.1385 - val_loss: 0.0281 - val_mae: 0.1237\n",
      "Epoch 59/250\n",
      "177/177 [==============================] - 15s 83ms/step - loss: 0.0225 - mae: 0.1132 - val_loss: 0.0282 - val_mae: 0.1371\n",
      "Epoch 60/250\n",
      "177/177 [==============================] - 15s 83ms/step - loss: 0.0215 - mae: 0.1119 - val_loss: 0.0457 - val_mae: 0.1804\n",
      "Epoch 61/250\n",
      "177/177 [==============================] - 15s 84ms/step - loss: 0.0244 - mae: 0.1160 - val_loss: 0.0219 - val_mae: 0.1217\n",
      "\n",
      "Now Training (71)\n",
      "{'length': 180, 'layers_num': 2, 'layers_type': 'LSTM', 'units': 80, 'g_filt': 2.5}\n",
      "Epoch 1/250\n",
      "177/177 [==============================] - 19s 87ms/step - loss: 48.4555 - mae: 5.4570 - val_loss: 3.4247 - val_mae: 1.5644\n",
      "Epoch 2/250\n",
      "177/177 [==============================] - 15s 83ms/step - loss: 4.4535 - mae: 1.5583 - val_loss: 1.5531 - val_mae: 1.0623\n",
      "Epoch 3/250\n",
      "177/177 [==============================] - 15s 82ms/step - loss: 1.2749 - mae: 0.8557 - val_loss: 0.6581 - val_mae: 0.6165\n",
      "Epoch 4/250\n",
      "177/177 [==============================] - 15s 83ms/step - loss: 0.8342 - mae: 0.6358 - val_loss: 0.3183 - val_mae: 0.4290\n",
      "Epoch 5/250\n",
      "177/177 [==============================] - 15s 83ms/step - loss: 0.4793 - mae: 0.4598 - val_loss: 0.3809 - val_mae: 0.5007\n",
      "Epoch 6/250\n",
      "177/177 [==============================] - 15s 84ms/step - loss: 0.3233 - mae: 0.4174 - val_loss: 0.2042 - val_mae: 0.3106\n",
      "Epoch 7/250\n",
      "177/177 [==============================] - 15s 83ms/step - loss: 0.1755 - mae: 0.3062 - val_loss: 0.3030 - val_mae: 0.4755\n",
      "Epoch 8/250\n",
      "177/177 [==============================] - 15s 84ms/step - loss: 0.1488 - mae: 0.2851 - val_loss: 0.2187 - val_mae: 0.3574\n",
      "Epoch 9/250\n",
      "177/177 [==============================] - 15s 84ms/step - loss: 0.1609 - mae: 0.2960 - val_loss: 0.4282 - val_mae: 0.5961\n",
      "Epoch 10/250\n",
      "177/177 [==============================] - 15s 83ms/step - loss: 0.2443 - mae: 0.3819 - val_loss: 0.0711 - val_mae: 0.1973\n",
      "Epoch 11/250\n",
      "177/177 [==============================] - 15s 83ms/step - loss: 0.0873 - mae: 0.2164 - val_loss: 0.1470 - val_mae: 0.3248\n",
      "Epoch 12/250\n",
      "177/177 [==============================] - 15s 84ms/step - loss: 0.0852 - mae: 0.2191 - val_loss: 0.0743 - val_mae: 0.2119\n",
      "Epoch 13/250\n",
      "177/177 [==============================] - 15s 83ms/step - loss: 0.0662 - mae: 0.1903 - val_loss: 0.1618 - val_mae: 0.3339\n",
      "Epoch 14/250\n",
      "177/177 [==============================] - 15s 83ms/step - loss: 0.0565 - mae: 0.1830 - val_loss: 0.0579 - val_mae: 0.1946\n",
      "Epoch 15/250\n",
      "177/177 [==============================] - 15s 83ms/step - loss: 0.0451 - mae: 0.1541 - val_loss: 0.1027 - val_mae: 0.2800\n",
      "Epoch 16/250\n",
      "177/177 [==============================] - 15s 84ms/step - loss: 0.0438 - mae: 0.1639 - val_loss: 0.0383 - val_mae: 0.1497\n",
      "Epoch 17/250\n",
      "177/177 [==============================] - 15s 83ms/step - loss: 0.0560 - mae: 0.1812 - val_loss: 0.0546 - val_mae: 0.1888\n",
      "Epoch 18/250\n",
      "177/177 [==============================] - 15s 84ms/step - loss: 0.0963 - mae: 0.2370 - val_loss: 0.1570 - val_mae: 0.3487\n",
      "Epoch 19/250\n",
      "177/177 [==============================] - 15s 84ms/step - loss: 0.0782 - mae: 0.2136 - val_loss: 0.0208 - val_mae: 0.1139\n",
      "Epoch 20/250\n",
      "177/177 [==============================] - 15s 83ms/step - loss: 0.0516 - mae: 0.1651 - val_loss: 0.1215 - val_mae: 0.3180\n",
      "Epoch 21/250\n",
      "177/177 [==============================] - 15s 83ms/step - loss: 0.0856 - mae: 0.2280 - val_loss: 0.0777 - val_mae: 0.2530\n",
      "Epoch 22/250\n",
      "177/177 [==============================] - 15s 83ms/step - loss: 0.0434 - mae: 0.1642 - val_loss: 0.0170 - val_mae: 0.1007\n",
      "Epoch 23/250\n",
      "177/177 [==============================] - 15s 83ms/step - loss: 0.0370 - mae: 0.1470 - val_loss: 0.0194 - val_mae: 0.1156\n",
      "Epoch 24/250\n",
      "177/177 [==============================] - 15s 84ms/step - loss: 0.0208 - mae: 0.1066 - val_loss: 0.0115 - val_mae: 0.0828\n",
      "Epoch 25/250\n",
      "177/177 [==============================] - 15s 83ms/step - loss: 0.0164 - mae: 0.0912 - val_loss: 0.0307 - val_mae: 0.1278\n",
      "Epoch 26/250\n",
      "177/177 [==============================] - 15s 83ms/step - loss: 0.0710 - mae: 0.2001 - val_loss: 0.0249 - val_mae: 0.1207\n",
      "Epoch 27/250\n",
      "177/177 [==============================] - 15s 83ms/step - loss: 0.0881 - mae: 0.1735 - val_loss: 0.1583 - val_mae: 0.3084\n",
      "Epoch 28/250\n",
      "177/177 [==============================] - 15s 83ms/step - loss: 0.1661 - mae: 0.3217 - val_loss: 0.1131 - val_mae: 0.2985\n",
      "Epoch 29/250\n",
      "177/177 [==============================] - 15s 83ms/step - loss: 0.0865 - mae: 0.2329 - val_loss: 0.0456 - val_mae: 0.1855\n",
      "Epoch 30/250\n",
      "177/177 [==============================] - 15s 83ms/step - loss: 0.0369 - mae: 0.1428 - val_loss: 0.0823 - val_mae: 0.2566\n",
      "Epoch 31/250\n",
      "177/177 [==============================] - 15s 84ms/step - loss: 0.0302 - mae: 0.1307 - val_loss: 0.0090 - val_mae: 0.0751\n",
      "Epoch 32/250\n",
      "177/177 [==============================] - 15s 83ms/step - loss: 0.0167 - mae: 0.0960 - val_loss: 0.0113 - val_mae: 0.0805\n",
      "Epoch 33/250\n",
      "177/177 [==============================] - 15s 84ms/step - loss: 0.0230 - mae: 0.1166 - val_loss: 0.0173 - val_mae: 0.0978\n",
      "Epoch 34/250\n",
      "177/177 [==============================] - 15s 83ms/step - loss: 0.0228 - mae: 0.1036 - val_loss: 0.0369 - val_mae: 0.1686\n",
      "Epoch 35/250\n",
      "177/177 [==============================] - 15s 83ms/step - loss: 0.0336 - mae: 0.1378 - val_loss: 0.0094 - val_mae: 0.0762\n",
      "Epoch 36/250\n",
      "177/177 [==============================] - 15s 84ms/step - loss: 0.0234 - mae: 0.1237 - val_loss: 0.0134 - val_mae: 0.1007\n",
      "Epoch 37/250\n",
      "177/177 [==============================] - 15s 83ms/step - loss: 0.0224 - mae: 0.1165 - val_loss: 0.0228 - val_mae: 0.1270\n",
      "Epoch 38/250\n",
      "177/177 [==============================] - 15s 84ms/step - loss: 0.0263 - mae: 0.1304 - val_loss: 0.0127 - val_mae: 0.1002\n",
      "Epoch 39/250\n",
      "177/177 [==============================] - 15s 84ms/step - loss: 0.0156 - mae: 0.0941 - val_loss: 0.0130 - val_mae: 0.0992\n",
      "Epoch 40/250\n",
      "177/177 [==============================] - 15s 84ms/step - loss: 0.0131 - mae: 0.0901 - val_loss: 0.0099 - val_mae: 0.0863\n",
      "Epoch 41/250\n",
      "177/177 [==============================] - 15s 83ms/step - loss: 0.0555 - mae: 0.1680 - val_loss: 0.0079 - val_mae: 0.0670\n",
      "Epoch 42/250\n",
      "177/177 [==============================] - 15s 83ms/step - loss: 0.0149 - mae: 0.0951 - val_loss: 0.0140 - val_mae: 0.1031\n",
      "Epoch 43/250\n",
      "177/177 [==============================] - 15s 83ms/step - loss: 0.0118 - mae: 0.0791 - val_loss: 0.0036 - val_mae: 0.0474\n",
      "Epoch 44/250\n",
      "177/177 [==============================] - 15s 83ms/step - loss: 0.0093 - mae: 0.0704 - val_loss: 0.0272 - val_mae: 0.1319\n",
      "Epoch 45/250\n",
      "177/177 [==============================] - 15s 83ms/step - loss: 0.0186 - mae: 0.1017 - val_loss: 0.1219 - val_mae: 0.2685\n",
      "Epoch 46/250\n",
      "177/177 [==============================] - 15s 84ms/step - loss: 0.1371 - mae: 0.2823 - val_loss: 0.0043 - val_mae: 0.0479\n",
      "Epoch 47/250\n",
      "177/177 [==============================] - 15s 83ms/step - loss: 0.0076 - mae: 0.0653 - val_loss: 0.0030 - val_mae: 0.0426\n",
      "Epoch 48/250\n",
      "177/177 [==============================] - 15s 83ms/step - loss: 0.0064 - mae: 0.0593 - val_loss: 0.0027 - val_mae: 0.0418\n",
      "Epoch 49/250\n",
      "177/177 [==============================] - 15s 82ms/step - loss: 0.0175 - mae: 0.0932 - val_loss: 0.0085 - val_mae: 0.0782\n",
      "Epoch 50/250\n",
      "177/177 [==============================] - 15s 82ms/step - loss: 0.0157 - mae: 0.0950 - val_loss: 0.0138 - val_mae: 0.1059\n",
      "Epoch 51/250\n",
      "177/177 [==============================] - 15s 84ms/step - loss: 0.0180 - mae: 0.1077 - val_loss: 0.0057 - val_mae: 0.0562\n",
      "Epoch 52/250\n",
      "177/177 [==============================] - 15s 83ms/step - loss: 0.0094 - mae: 0.0755 - val_loss: 0.0030 - val_mae: 0.0410\n",
      "Epoch 53/250\n",
      "177/177 [==============================] - 15s 83ms/step - loss: 0.0038 - mae: 0.0439 - val_loss: 0.0018 - val_mae: 0.0323\n",
      "Epoch 54/250\n",
      "177/177 [==============================] - 15s 83ms/step - loss: 0.0420 - mae: 0.1456 - val_loss: 0.0312 - val_mae: 0.1183\n",
      "Epoch 55/250\n",
      "177/177 [==============================] - 15s 83ms/step - loss: 0.0105 - mae: 0.0753 - val_loss: 0.4259 - val_mae: 0.6133\n",
      "Epoch 56/250\n",
      "177/177 [==============================] - 15s 84ms/step - loss: 0.2835 - mae: 0.4080 - val_loss: 0.0059 - val_mae: 0.0549\n",
      "Epoch 57/250\n",
      "177/177 [==============================] - 15s 83ms/step - loss: 0.0111 - mae: 0.0752 - val_loss: 0.0065 - val_mae: 0.0700\n",
      "Epoch 58/250\n",
      "177/177 [==============================] - 15s 84ms/step - loss: 0.0099 - mae: 0.0768 - val_loss: 0.0019 - val_mae: 0.0328\n",
      "Epoch 59/250\n",
      "177/177 [==============================] - 15s 83ms/step - loss: 0.0051 - mae: 0.0540 - val_loss: 0.0019 - val_mae: 0.0343\n",
      "Epoch 60/250\n",
      "177/177 [==============================] - 15s 83ms/step - loss: 0.0218 - mae: 0.1152 - val_loss: 0.0221 - val_mae: 0.0929\n",
      "Epoch 61/250\n",
      "177/177 [==============================] - 15s 83ms/step - loss: 0.0377 - mae: 0.1414 - val_loss: 0.0124 - val_mae: 0.0919\n",
      "Epoch 62/250\n",
      "177/177 [==============================] - 15s 83ms/step - loss: 0.0217 - mae: 0.1154 - val_loss: 0.0032 - val_mae: 0.0421\n",
      "Epoch 63/250\n",
      "177/177 [==============================] - 15s 84ms/step - loss: 0.0067 - mae: 0.0615 - val_loss: 0.0101 - val_mae: 0.0908\n",
      "Epoch 64/250\n",
      "177/177 [==============================] - 15s 83ms/step - loss: 0.0285 - mae: 0.1343 - val_loss: 0.0121 - val_mae: 0.0909\n",
      "Epoch 65/250\n",
      "177/177 [==============================] - 15s 83ms/step - loss: 0.0116 - mae: 0.0781 - val_loss: 0.0235 - val_mae: 0.1364\n",
      "Epoch 66/250\n",
      "177/177 [==============================] - 15s 83ms/step - loss: 0.0432 - mae: 0.1561 - val_loss: 0.0614 - val_mae: 0.2304\n",
      "Epoch 67/250\n",
      "177/177 [==============================] - 15s 82ms/step - loss: 0.0366 - mae: 0.1483 - val_loss: 0.0112 - val_mae: 0.0707\n",
      "Epoch 68/250\n",
      "177/177 [==============================] - 15s 83ms/step - loss: 0.0306 - mae: 0.1362 - val_loss: 0.0093 - val_mae: 0.0742\n",
      "Epoch 69/250\n",
      "177/177 [==============================] - 15s 83ms/step - loss: 0.0052 - mae: 0.0551 - val_loss: 0.0100 - val_mae: 0.0840\n",
      "Epoch 70/250\n",
      "177/177 [==============================] - 15s 83ms/step - loss: 0.0059 - mae: 0.0580 - val_loss: 0.0033 - val_mae: 0.0473\n",
      "Epoch 71/250\n",
      "177/177 [==============================] - 15s 83ms/step - loss: 0.0331 - mae: 0.1318 - val_loss: 0.0019 - val_mae: 0.0312\n",
      "Epoch 72/250\n",
      "177/177 [==============================] - 15s 82ms/step - loss: 0.0033 - mae: 0.0434 - val_loss: 0.0018 - val_mae: 0.0367\n",
      "Epoch 73/250\n",
      "177/177 [==============================] - 15s 82ms/step - loss: 0.0039 - mae: 0.0438 - val_loss: 0.0135 - val_mae: 0.0924\n"
     ]
    }
   ],
   "source": [
    "length = [30, 90, 180]\n",
    "layers_num = [1, 2]\n",
    "layers_type = ['LSTM']\n",
    "units = [20, 40, 80] \n",
    "g_filt = [1, 1.5, 2, 2.5]\n",
    "\n",
    "model_name = 'temp_model.h5'\n",
    "\n",
    "grid_table = gridTableGen(length, layers_num, layers_type, units, g_filt)\n",
    "results = gridSearch(grid_table, temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BEQU121JBHya"
   },
   "source": [
    "## Analyse the Grid Search Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YlsPbt8oDqrd"
   },
   "outputs": [],
   "source": [
    "#load it from drive\n",
    "gs = pd.read_csv('results_table_temp_smooth.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 875,
     "status": "ok",
     "timestamp": 1621887990175,
     "user": {
      "displayName": "1990JAF",
      "photoUrl": "",
      "userId": "13601147066600634770"
     },
     "user_tz": -60
    },
    "id": "GzHiarNMBL8y",
    "outputId": "74600356-ed14-4594-fd8a-f66d3afb8444"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEoCAYAAABILwrfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAQpElEQVR4nO3df7DldV3H8ecLVqBCsdzrj/YHS7WWW1rgDVHTMEEXVHYmlVhlDGRcm4nUNBLLoaKyrEYndVN3Ukj8gahJaywuhphW4rCAIgth1xXdXc1dEQz8BSvv/jhfnMPl3r3n7p69x/u5z8fMHb4/PnvOe3fgyXe/555zU1VIkua/g0Y9gCRpOAy6JDXCoEtSIwy6JDXCoEtSIwy6JDXCoEtSIwy6JDXCoEtSIwy6mpPk1iTnJLkhybeTvCPJI5JcnuTOJP+W5Ce7tccl+a8kdyT5XJLj+x7nzCQ3d79mW5KX9p07PsmOJK9KsivJ15KcOYLfrvRDBl2tei5wIvBo4DnA5cAfAWP0/r1/WZIlwGXAXwA/BfwB8KEkY91j7AKeDTwEOBN4Y5Jj+p7jkcARwBLgLGD9ff+jkEbBoKtVb66qr1fVTuBTwGeq6vqq+h7wYeBo4HRgU1Vtqqp7q+pjwBbgZICquqyqvlg9/w5cATyl7znuAc6vqnuqahNwF/Dzc/dblO7PoKtVX+/b/u4U+4cDRwLP72633JHkDuDXgEcBJDkpydVJvtmdOxlY3Pc4t1XVnr7973SPK43EolEPII3QduCiqnrJ5BNJDgU+BLwI+JequifJpUDmeEZpYF6hayF7N/CcJM9McnCSw7oXO5cChwCHAruBPUlOAp4xymGlmRh0LVhVtR1YQ+/F0t30rtjPAQ6qqjuBlwGXALcDLwA2jmhUaSDxB1xIUhu8QpekRhh0SWqEQZekRhh0SWqEQZekRozsjUWLFy+uFStWjOrpJWleuvbaa79RVWNTnRtZ0FesWMGWLVtG9fSSNC8l+fJ057zlIkmNMOiS1AiDLkmNMOiS1AiDLkmNMOiS1AiDLkmNMOiS1Ah/BF0jVpx72ahHkKZ0618/a9QjLBheoUtSIwy6JDXCoEtSIwy6JDXCoEtSIwy6JDXCoEtSIwy6JDXCoEtSIwy6JDVixqAneWeSXUlunOZ8krwpyUSSG5IcM/wxJUkzGeQK/UJg9V7OnwSs7L7WAW/d/7EkSbM1Y9Cr6pPAN/eyZA3wruq5GnhokkcNa0BJ0mCGcQ99CbC9b39Hd0ySNIfm9EXRJOuSbEmyZffu3XP51JLUvGEEfSewrG9/aXfsAapqQ1WNV9X42NjYEJ5aknSfYQR9I/Ci7rtdjgO+VVVfG8LjSpJmYcafWJTkfcDxwOIkO4A/AR4EUFVvAzYBJwMTwHeAMw/UsJKk6c0Y9KpaO8P5An53aBNJkvaJ7xSVpEYYdElqhEGXpEYYdElqhEGXpEYYdElqhEGXpEYYdElqhEGXpEYYdElqhEGXpEYYdElqhEGXpEYYdElqhEGXpEYYdElqhEGXpEYYdElqhEGXpEYYdElqhEGXpEYYdElqhEGXpEYYdElqhEGXpEYYdElqhEGXpEYYdElqhEGXpEYYdElqxEBBT7I6yS1JJpKcO8X55UmuSnJ9khuSnDz8USVJezNj0JMcDKwHTgJWAWuTrJq07LXAJVV1NHAa8A/DHlSStHeDXKEfC0xU1baquhu4GFgzaU0BD+m2jwC+OrwRJUmDWDTAmiXA9r79HcATJq35U+CKJL8H/ARwwlCmkyQNbFgviq4FLqyqpcDJwEVJHvDYSdYl2ZJky+7du4f01JIkGCzoO4FlfftLu2P9zgIuAaiqTwOHAYsnP1BVbaiq8aoaHxsb27eJJUlTGiTo1wArkxyV5BB6L3punLTmK8DTAZI8hl7QvQSXpDk0Y9Crag9wNrAZuJned7NsTXJ+klO6Za8CXpLkc8D7gDOqqg7U0JKkBxrkRVGqahOwadKx8/q2bwKePNzRJEmz4TtFJakRBl2SGmHQJakRBl2SGmHQJakRBl2SGmHQJakRBl2SGmHQJakRBl2SGmHQJakRBl2SGmHQJakRBl2SGmHQJakRBl2SGmHQJakRBl2SGmHQJakRBl2SGmHQJakRBl2SGmHQJakRBl2SGmHQJakRBl2SGmHQJakRBl2SGmHQJakRAwU9yeoktySZSHLuNGtOTXJTkq1J3jvcMSVJM1k004IkBwPrgROBHcA1STZW1U19a1YCrwGeXFW3J3n4gRpYkjS1Qa7QjwUmqmpbVd0NXAysmbTmJcD6qrodoKp2DXdMSdJMBgn6EmB73/6O7li/RwOPTvKfSa5OsnpYA0qSBjPjLZdZPM5K4HhgKfDJJI+tqjv6FyVZB6wDWL58+ZCeWpIEg12h7wSW9e0v7Y712wFsrKp7qupLwBfoBf5+qmpDVY1X1fjY2Ni+zixJmsIgQb8GWJnkqCSHAKcBGyetuZTe1TlJFtO7BbNtiHNKkmYwY9Crag9wNrAZuBm4pKq2Jjk/ySndss3AbUluAq4Czqmq2w7U0JKkBxroHnpVbQI2TTp2Xt92Aa/sviRJI+A7RSWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhoxUNCTrE5yS5KJJOfuZd1zk1SS8eGNKEkaxIxBT3IwsB44CVgFrE2yaop1DwZeDnxm2ENKkmY2yBX6scBEVW2rqruBi4E1U6z7c+D1wPeGOJ8kaUCDBH0JsL1vf0d37IeSHAMsq6rLhjibJGkW9vtF0SQHAW8AXjXA2nVJtiTZsnv37v19aklSn0GCvhNY1re/tDt2nwcDvwR8IsmtwHHAxqleGK2qDVU1XlXjY2Nj+z61JOkBBgn6NcDKJEclOQQ4Ddh438mq+lZVLa6qFVW1ArgaOKWqthyQiSVJU5ox6FW1Bzgb2AzcDFxSVVuTnJ/klAM9oCRpMIsGWVRVm4BNk46dN83a4/d/LEnSbPlOUUlqhEGXpEYYdElqhEGXpEYYdElqhEGXpEYYdElqhEGXpEYYdElqhEGXpEYYdElqhEGXpEYYdElqhEGXpEYYdElqhEGXpEYYdElqhEGXpEYYdElqhEGXpEYYdElqhEGXpEYYdElqhEGXpEYYdElqhEGXpEYYdElqhEGXpEYYdElqxEBBT7I6yS1JJpKcO8X5Vya5KckNSa5McuTwR5Uk7c2MQU9yMLAeOAlYBaxNsmrSsuuB8ap6HPBB4G+GPagkae8GuUI/Fpioqm1VdTdwMbCmf0FVXVVV3+l2rwaWDndMSdJMBgn6EmB73/6O7th0zgIu35+hJEmzt2iYD5bkdGAc+PVpzq8D1gEsX758mE8tSQveIFfoO4FlfftLu2P3k+QE4I+BU6rq+1M9UFVtqKrxqhofGxvbl3klSdMYJOjXACuTHJXkEOA0YGP/giRHA2+nF/Ndwx9TkjSTGYNeVXuAs4HNwM3AJVW1Ncn5SU7plv0tcDjwgSSfTbJxmoeTJB0gA91Dr6pNwKZJx87r2z5hyHNJkmbJd4pKUiMMuiQ1wqBLUiMMuiQ1wqBLUiMMuiQ1wqBLUiMMuiQ1wqBLUiMMuiQ1wqBLUiMMuiQ1wqBLUiMMuiQ1wqBLUiMMuiQ1wqBLUiMMuiQ1wqBLUiMMuiQ1wqBLUiMMuiQ1wqBLUiMMuiQ1wqBLUiMMuiQ1wqBLUiMMuiQ1wqBLUiMMuiQ1YqCgJ1md5JYkE0nOneL8oUne353/TJIVwx5UkrR3MwY9ycHAeuAkYBWwNsmqScvOAm6vqp8D3gi8ftiDSpL2bpAr9GOBiaraVlV3AxcDayatWQP8U7f9QeDpSTK8MSVJMxkk6EuA7X37O7pjU66pqj3At4CHDWNASdJgFs3lkyVZB6zrdu9KcstcPr80oMXAN0Y9RCviDdhhO3K6E4MEfSewrG9/aXdsqjU7kiwCjgBum/xAVbUB2DDAc0ojk2RLVY2Peg5ptga55XINsDLJUUkOAU4DNk5asxH47W77ecDHq6qGN6YkaSYzXqFX1Z4kZwObgYOBd1bV1iTnA1uqaiPwDuCiJBPAN+lFX5I0h+KFtHR/SdZ1twelecWgS1IjfOu/JDXCoEtSIwy6JDViTt9YJP2oSfLKvZ2vqjfM1SzS/jLoWuj+DvgscDnwfcDPINK8ZdC10B0NrAWeBVwLvA+40jfGaT7y2xalTpIn0Yv7CcCruzfNSfOGL4pKQJIxelfrj6X3iaK7RjuRNHvectGCluTFwKnAYfQ+y//UqjLmmpe85aIFLcm9wI3Al7tD9/sPoqpOmfOhpH3kFboWuqeNegBpWAy6Frozq+qMUQ8hDYMvimqhe9yoB5CGxSt0LXQ/nuRopnlDUVVdN8fzSPvMF0W1oCW5k95P5Zoq6FVVvzHHI0n7zCt0LXQTRlut8B66JDXCoGuhe3X/TpIHJTk6ycNHNZC0rwy6FrrfTPKLAEmOAD4HvAu4PsnakU4mzZJB10L3lKra2m2fCXyhqh4LPB74w9GNJc2eQddCd3ff9onApQBV9b+jGUfadwZdC90dSZ7dfS/6k4GPAiRZBPzYSCeTZslvW9RC91LgTcAjgVf0XZk/HbhsZFNJ+8Cga0Grqi8Aq6c4vjnJY0YwkrTPfKeoNI0kX6mq5aOeQxqU99Cl6fkDozWvGHRpev71VfOK99C1oHUfzjVVuIPf5aJ5xnvoktQIb7lIUiMMuiQ1wqBLUiMMuuaFJHeNeob7JDkjyU+Peg5pMoOuBS89s/lv4QzAoOtHjkHXvJLk8CRXJrkuyeeTrOmOn5/kFX3r/jLJy7vtc5Jck+SGJH/WHVuR5JYk7wJuBJYluTDJjd3j/v40z/88YBx4T5LPJnlWkkv7zp+Y5MPd9l1J3phkazfzWHf8Z5N8NMm1ST6V5BcOzJ+WFpyq8suvH/kv4K7un4uAh3Tbi4EJet8zvgK4rjt+EPBF4GHAM4AN3ZqDgH8Fntqtvxc4rvs1jwc+1vd8D93LLJ8AxrvtAP8NjHX77wWe020X8MJu+zzgLd32lcDKbvsJwMdH/efrVxtfvrFI802A1yV5Kr0gLwEeUVW3Jrmt+xjcRwDXV9VtSZ5BL+rXd7/+cGAl8BXgy1V1dXd8G/AzSd5M71MWrxhkmKqqJBcBpye5AHgi8KLu9L3A+7vtdwP/nORw4EnAB5IffrLAobP+U5CmYNA137wQGAMeX1X3JLkVOKw794/07m8/EnhndyzAX1XV2/sfJMkK4Nv37VfV7Ul+GXgm8DvAqcCLB5zpAuAjwPeAD1TVnmnWFb2/JdxRVb8y4GNLA/MeuuabI4BdXcyfBhzZd+7D9D4K91eBzd2xzcCLuytjkiyZ6gdAJ1kMHFRVHwJeCxyzlxnuBB58305VfRX4avfrLuhbdxDwvG77BcB/VNX/AV9K8vzuedP9j0Tab16ha755D/CRJJ8HttC7fw1AVd2d5Cp6V8A/6I5d0X2u+ae7Wxx3AacDP5j0uEuAC/q+2+U1e5nhQuBtSb4LPLGqvtvNNVZVN/et+zZwbJLXAruA3+qOvxB4a3f8QcDF9H44tbRf/CwXNaOL8XXA86vqf+b4ud9C7779O/qO3VVVh8/lHFrYvOWiJiRZRe87Xq4cQcyvBR5H74VPaWS8QpemkWQ9vR8c3e/vq+qCqdZLo2bQJakR3nKRpEYYdElqhEGXpEYYdElqhEGXpEb8PxqFvVgplb9KAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEdCAYAAAAcmJzBAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAARR0lEQVR4nO3df5BdZ13H8fenSUuFIgpZKyZpt2pQMwJtZ6lVdKgDSJJCowNKo1jpFMKopTr8GKJicQrjgDgCaqBkpBbq2FqLlkgS669ildra7Q9Kf0wxlEISmXYpbYdaa5vh6x97Czfb3dyb7Mne7pP3ayYz95znyTnffZLzyZPz455UFZKkxe+IURcgSeqGgS5JjTDQJakRBrokNcJAl6RGGOiS1AgDXZIaYaBLUiMMdElqhIGuJiW5J8nbk9ya5H+SfCzJsUl2JPlGkn9K8t29vqcmuTbJg0k+l+S0vu2cneTO3u+5O8mb+tpOS7I7yVuT3Jfkq0nOHsGPKwEGutr2auDlwPOAVwE7gN8Gxpj+u39ekuXANuA9wLOBtwGfTDLW28Z9wCuB7wTOBj6Q5OS+fXwv8CxgOXAOsPmJfyikhWagq2V/UlX3VtUe4N+A66vq5qp6FPhb4CTgdcD2qtpeVd+sqn8EJoF1AFW1raq+WNP+FfgH4Kf69vE4cEFVPV5V24GHgR9auB9R+jYDXS27t+/z/86yfAxwPPDzvdMtDyZ5EPhJ4LkASdYmuS7J13tt64Blfdu5v6r29i0/0tuutOCWjroAacR2AZdU1RtnNiR5GvBJ4CzgU1X1eJIrgSxwjdJQnKHrcPcXwKuSvCLJkiRH9y52rgCOAp4GTAF7k6wFfmaUxUr7Y6DrsFZVu4D1TF8snWJ6xv524Iiq+gZwHnA58ADwi8DWEZUqDRRfcCFJbXCGLkmNMNAlqREGuiQ1wkCXpEaM7D70ZcuW1fj4+Kh2L0mL0o033vi1qhqbrW1kgT4+Ps7k5OSodi9Ji1KSL8/V5ikXSWqEgS5JjTDQJakRBrokNcJAl6RGGOiS1AgDXZIaMTDQk1zUewHubQP6vSjJ3iSv6a48SdKwhpmhXwys2V+HJEuA9zH9vkVJ0ggMfFK0qq5JMj6g25uZflXXizqoSZL2a3zTtlGXMJR73nv6gu5v3ufQkywHfg74yBB9NyaZTDI5NTU1311Lkvp0cVH0g8A7quqbgzpW1ZaqmqiqibGxWb9bRpJ0kLr4cq4J4LIkAMuAdUn2VtWVHWxbkjSkeQd6VZ3wxOckFwOfNswlaeENDPQklwKnAcuS7AbeBRwJUFUXHtLqJElDG+Yulw3DbqyqXj+vaiRJB80nRSWpEQa6JDXCQJekRhjoktQIA12SGmGgS1IjDHRJaoSBLkmNMNAlqREGuiQ1wkCXpEYY6JLUCANdkhphoEtSIwx0SWqEgS5JjejinaKShjC+aduoSxjonveePuoSNA/O0CWpEQa6JDXCQJekRgwM9CQXJbkvyW1ztP9SkluTfD7JtUle2H2ZkqRBhpmhXwys2U/7l4CXVNXzgXcDWzqoS5J0gAbe5VJV1yQZ30/7tX2L1wEr5l+WJOlAdX0O/Rxgx1yNSTYmmUwyOTU11fGuJenw1lmgJ/lppgP9HXP1qaotVTVRVRNjY2Nd7VqSREcPFiV5AfBnwNqqur+LbUqSDsy8Z+hJjgP+BvjlqvrC/EuSJB2MgTP0JJcCpwHLkuwG3gUcCVBVFwLnA88BPpwEYG9VTRyqgiVJsxvmLpcNA9rfALyhs4okSQfFJ0UlqREGuiQ1wkCXpEYY6JLUCANdkhphoEtSIwx0SWqEgS5JjTDQJakRBrokNcJAl6RGGOiS1AgDXZIaYaBLUiMMdElqhIEuSY0w0CWpEQa6JDXCQJekRhjoktSIgYGe5KIk9yW5bY72JPnjJDuT3Jrk5O7LlCQNMswM/WJgzX7a1wKrer82Ah+Zf1mSpAO1dFCHqromyfh+uqwHPlFVBVyX5LuSPLeqvtpRjRqR8U3bRl3CUO557+mjLkF6SujiHPpyYFff8u7eOknSAlrQi6JJNiaZTDI5NTW1kLuWpOZ1Eeh7gJV9yyt6656kqrZU1URVTYyNjXWwa0nSE7oI9K3AWb27XU4FHvL8uSQtvIEXRZNcCpwGLEuyG3gXcCRAVV0IbAfWATuBR4CzD1WxkqS5DXOXy4YB7QX8emcVSZIOik+KSlIjDHRJaoSBLkmNMNAlqREGuiQ1wkCXpEYY6JLUCANdkhphoEtSIwx0SWqEgS5JjTDQJakRBrokNcJAl6RGGOiS1AgDXZIaYaBLUiMMdElqhIEuSY0w0CWpEQa6JDViqEBPsibJXUl2Jtk0S/txSa5OcnOSW5Os675USdL+DAz0JEuAzcBaYDWwIcnqGd3eCVxeVScBZwIf7rpQSdL+DTNDPwXYWVV3V9VjwGXA+hl9CvjO3udnAf/dXYmSpGEME+jLgV19y7t76/r9HvC6JLuB7cCbZ9tQko1JJpNMTk1NHUS5kqS5dHVRdANwcVWtANYBlyR50raraktVTVTVxNjYWEe7liTBcIG+B1jZt7yit67fOcDlAFX1H8DRwLIuCpQkDWeYQL8BWJXkhCRHMX3Rc+uMPl8BXgqQ5EeYDnTPqUjSAhoY6FW1FzgXuAq4k+m7WW5PckGSM3rd3gq8McnngEuB11dVHaqiJUlPtnSYTlW1nemLnf3rzu/7fAfw4m5LkyQdCJ8UlaRGGOiS1AgDXZIaYaBLUiMMdElqhIEuSY0w0CWpEQa6JDXCQJekRhjoktQIA12SGmGgS1IjDHRJaoSBLkmNMNAlqRFDfR/6YjG+aduoSxjKPe89fdQlSGqQM3RJaoSBLkmNMNAlqREGuiQ1YqhAT7ImyV1JdibZNEefX0hyR5Lbk/xlt2VKkgYZeJdLkiXAZuDlwG7ghiRbq+qOvj6rgN8CXlxVDyT5nkNVsCRpdsPM0E8BdlbV3VX1GHAZsH5GnzcCm6vqAYCquq/bMiVJgwwT6MuBXX3Lu3vr+j0PeF6Szya5LsmargqUJA2nqweLlgKrgNOAFcA1SZ5fVQ/2d0qyEdgIcNxxx3W0a0kSDDdD3wOs7Fte0VvXbzewtaoer6ovAV9gOuD3UVVbqmqiqibGxsYOtmZJ0iyGCfQbgFVJTkhyFHAmsHVGnyuZnp2TZBnTp2Du7rBOSdIAAwO9qvYC5wJXAXcCl1fV7UkuSHJGr9tVwP1J7gCuBt5eVfcfqqIlSU821Dn0qtoObJ+x7vy+zwW8pfdLkjQCPikqSY0w0CWpEQa6JDXCQJekRhjoktQIA12SGmGgS1IjDHRJaoSBLkmNMNAlqREGuiQ1wkCXpEYY6JLUCANdkhphoEtSIwx0SWqEgS5JjTDQJakRBrokNcJAl6RGGOiS1IihAj3JmiR3JdmZZNN++r06SSWZ6K5ESdIwBgZ6kiXAZmAtsBrYkGT1LP2eCfwGcH3XRUqSBhtmhn4KsLOq7q6qx4DLgPWz9Hs38D7g0Q7rkyQNaZhAXw7s6lve3Vv3LUlOBlZW1bb9bSjJxiSTSSanpqYOuFhJ0tzmfVE0yRHAHwFvHdS3qrZU1URVTYyNjc1315KkPsME+h5gZd/yit66JzwT+FHgM0nuAU4FtnphVJIW1jCBfgOwKskJSY4CzgS2PtFYVQ9V1bKqGq+qceA64IyqmjwkFUuSZjUw0KtqL3AucBVwJ3B5Vd2e5IIkZxzqAiVJw1k6TKeq2g5sn7Hu/Dn6njb/siRJB8onRSWpEQa6JDXCQJekRhjoktQIA12SGmGgS1IjDHRJaoSBLkmNMNAlqREGuiQ1wkCXpEYY6JLUCANdkhphoEtSIwx0SWqEgS5JjTDQJakRBrokNcJAl6RGGOiS1IihAj3JmiR3JdmZZNMs7W9JckeSW5P8c5Ljuy9VkrQ/AwM9yRJgM7AWWA1sSLJ6RrebgYmqegFwBfAHXRcqSdq/YWbopwA7q+ruqnoMuAxY39+hqq6uqkd6i9cBK7otU5I0yDCBvhzY1be8u7duLucAO2ZrSLIxyWSSyampqeGrlCQN1OlF0SSvAyaA98/WXlVbqmqiqibGxsa63LUkHfaWDtFnD7Cyb3lFb90+krwM+B3gJVX1f92UJ0ka1jAz9BuAVUlOSHIUcCawtb9DkpOAjwJnVNV93ZcpSRpkYKBX1V7gXOAq4E7g8qq6PckFSc7odXs/cAzw10luSbJ1js1Jkg6RYU65UFXbge0z1p3f9/llHdclSTpAPikqSY0w0CWpEQa6JDXCQJekRhjoktQIA12SGmGgS1IjDHRJaoSBLkmNMNAlqREGuiQ1wkCXpEYY6JLUCANdkhphoEtSIwx0SWqEgS5JjTDQJakRBrokNcJAl6RGGOiS1IihAj3JmiR3JdmZZNMs7U9L8le99uuTjHddqCRp/wYGepIlwGZgLbAa2JBk9Yxu5wAPVNUPAh8A3td1oZKk/Rtmhn4KsLOq7q6qx4DLgPUz+qwHPt77fAXw0iTprkxJ0iBLh+izHNjVt7wb+LG5+lTV3iQPAc8BvtbfKclGYGNv8eEkdx1M0QtsGTN+jvnK4f3/F8ezO45ltxbLeB4/V8Mwgd6ZqtoCbFnIfc5Xksmqmhh1Ha1wPLvjWHarhfEc5pTLHmBl3/KK3rpZ+yRZCjwLuL+LAiVJwxkm0G8AViU5IclRwJnA1hl9tgK/0vv8GuBfqqq6K1OSNMjAUy69c+LnAlcBS4CLqur2JBcAk1W1FfgYcEmSncDXmQ79ViyqU0SLgOPZHceyW4t+PONEWpLa4JOiktQIA12SGmGgS1IjDHRJasSCPlgkaX6SHMv0k9kAe6rq3lHW05Ikz66qr4+6jvnwLpdZeNAcOi0cNKOQ5ETgQqYf2nviwb4VwIPAr1XVTaOqbTFK8s6qek/v82rgSuBIIMBrq+r6UdZ3sAz0Ph403Wr1oBmFJLcAb5o5ZklOBT5aVS8cTWWLU5Kbqurk3udtwJ9W1Y4kpwAfrKqfGG2FB8dA7+NB061WD5pRSPJfVbVqjradva+u1pBm/N28uapO6mvbZ3kx8Rz6vp4x26yxqq5L8oxRFNSQ76uqHQBV9Z9JvmPUBS0yO3r/KH6Cb3/76UrgLODvR1bV4vX9SbYy/b/FFUmeXlWP9NqOHGFd82Kg78uDpltNHjSjUFXnJVnL9LsHvnV9B9hcVdtHV9miNfOdDkfAt66ffWThy+mGp1xmmOOg2epBc+CSvGTGqhur6uHeQfOaqto8irqkVhno0iKXZGPvXQPqwGIeTx8sGlLvbUvqiOPZKV/32K1FO54G+vAW7R/yU5TjeYCS/HCSlyY5ZkbTl0dS0CLX4nga6MN7bNQFNMbxPABJzgM+BbwZuC1J/0W93x9NVYtXq+PpOfQhJflKVR036jpa4XgemCSfB368d1F5HLgCuKSqPrSY75selVbH09sW+yS5da4m4NiFrKUFjmenjqiqhwGq6p4kpwFXJDkeT18djCbH00Df17HAK4AHZqwPcO3Cl7PoOZ7duTfJiVV1C0BvZvlK4CLg+aMtbVFqcjwN9H19GjjmiT/kfkk+s/DlLHqOZ3fOAvb2r6iqvcBZST46mpIWtSbH03PoktQI73KRpEYY6JLUCANdkhphoEsz9J4gvCXJzUl+IMm1vfXjSW7rfT4xybrRVirty0CXnuxngSuq6qSq+uIcL+I4ETDQ9ZRioOuwkeR3k9yV5N+TXJrkbbP0WQf8JvCrSa7urXt4Rp+jgAuA1/Zm8q9diPqlQbwPXYeFJC8CXg28kOmXa9wE3DizX1VtT3Ih8HBV/eFs26qqx5KcD0xU1bmHsGzpgBjoOly8GPhUVT0KPJrk70ZdkNQ1T7lIUiMMdB0uPgu8KsnRve+/fuU8t/cN4JnzL0vqjoGuw0JV3QBsBW4FdgCfBx6axyavBlZ7UVRPJX6Xiw4bSY7pfave04FrgI1VddOo65K64kVRHU62JFkNHA183DBXa5yh67CVZDPTd7/0+1BV/fko6pHmy0CXpEZ4UVSSGmGgS1IjDHRJaoSBLkmN+H+0Ss0N0EcVxAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "gs.groupby('length')['val_mae_og'].mean().plot(kind='bar', title='mean'); plt.show()\n",
    "gs.groupby('layers_num')['val_mae_og'].mean().plot(kind='bar', title='mean'); plt.show()\n",
    "gs.groupby('layers_type')['val_mae_og'].mean().plot(kind='bar', title='mean'); plt.show()\n",
    "gs.groupby('units')['val_mae_og'].mean().plot(kind='bar', title='mean'); plt.show()\n",
    "gs.groupby('g_filt')['val_mae_og'].mean().plot(kind='bar', title='mean'); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KkBeoVk4BVXY"
   },
   "source": [
    "That's much better compared to last time!\n",
    "\n",
    "The model was more able to pick out the trends it needed with less noise in the data.\n",
    "\n",
    "Performance is mostly influenced by our smoothing factor.\n",
    "\n",
    "The other hyperparameters are of little consequence it seems.\n",
    "\n",
    "Performance decreases the more we smooth. The model fits onto something that\n",
    "is too far removed from the real series. The signal is altered so much that it does not reflect the underlying data anymore.\n",
    "\n",
    "So we need some smoothing but not too much!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EwOTcZumBLeK"
   },
   "source": [
    "## Recreate the best model and compare against test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 174,
     "status": "ok",
     "timestamp": 1621888149039,
     "user": {
      "displayName": "1990JAF",
      "photoUrl": "",
      "userId": "13601147066600634770"
     },
     "user_tz": -60
    },
    "id": "MtCi3hH_BLt2",
    "outputId": "9423fbc4-9109-474f-a856-2b4e7a2ca36a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "length               30\n",
       "layers_num            1\n",
       "layers_type        LSTM\n",
       "units                80\n",
       "g_filt                1\n",
       "loss             0.1754\n",
       "mae              0.3299\n",
       "val_loss         0.1701\n",
       "val_mae          0.3237\n",
       "val_mae_og     0.635011\n",
       "epochs               61\n",
       "Name: 8, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model_params = gs.sort_values('val_mae_og').iloc[0]\n",
    "best_model_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 110462,
     "status": "ok",
     "timestamp": 1621888395784,
     "user": {
      "displayName": "1990JAF",
      "photoUrl": "",
      "userId": "13601147066600634770"
     },
     "user_tz": -60
    },
    "id": "67nu4yHUBelX",
    "outputId": "42306e1d-ca7b-46b6-bb6e-210ae7d34a7b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/120\n",
      "192/192 [==============================] - 34s 7ms/step - loss: 53.5142 - mae: 5.6720 - val_loss: 7.5169 - val_mae: 2.1209\n",
      "Epoch 2/120\n",
      "192/192 [==============================] - 1s 5ms/step - loss: 5.4025 - mae: 1.8085 - val_loss: 3.6308 - val_mae: 1.3941\n",
      "Epoch 3/120\n",
      "192/192 [==============================] - 1s 5ms/step - loss: 2.4684 - mae: 1.1938 - val_loss: 2.3300 - val_mae: 1.1267\n",
      "Epoch 4/120\n",
      "192/192 [==============================] - 1s 5ms/step - loss: 1.7580 - mae: 0.9879 - val_loss: 1.7809 - val_mae: 0.9747\n",
      "Epoch 5/120\n",
      "192/192 [==============================] - 1s 5ms/step - loss: 1.5430 - mae: 0.9333 - val_loss: 1.4295 - val_mae: 0.9045\n",
      "Epoch 6/120\n",
      "192/192 [==============================] - 1s 5ms/step - loss: 1.1294 - mae: 0.8046 - val_loss: 1.1499 - val_mae: 0.8023\n",
      "Epoch 7/120\n",
      "192/192 [==============================] - 1s 5ms/step - loss: 0.9144 - mae: 0.7257 - val_loss: 0.8554 - val_mae: 0.6867\n",
      "Epoch 8/120\n",
      "192/192 [==============================] - 1s 5ms/step - loss: 0.6593 - mae: 0.6124 - val_loss: 0.7557 - val_mae: 0.6576\n",
      "Epoch 9/120\n",
      "192/192 [==============================] - 1s 5ms/step - loss: 0.5574 - mae: 0.5710 - val_loss: 0.5998 - val_mae: 0.5797\n",
      "Epoch 10/120\n",
      "192/192 [==============================] - 1s 5ms/step - loss: 0.5282 - mae: 0.5513 - val_loss: 0.4988 - val_mae: 0.5250\n",
      "Epoch 11/120\n",
      "192/192 [==============================] - 1s 5ms/step - loss: 0.3896 - mae: 0.4657 - val_loss: 0.4654 - val_mae: 0.5142\n",
      "Epoch 12/120\n",
      "192/192 [==============================] - 1s 5ms/step - loss: 0.3639 - mae: 0.4640 - val_loss: 0.3743 - val_mae: 0.4590\n",
      "Epoch 13/120\n",
      "192/192 [==============================] - 1s 5ms/step - loss: 0.2936 - mae: 0.4176 - val_loss: 0.3354 - val_mae: 0.4374\n",
      "Epoch 14/120\n",
      "192/192 [==============================] - 1s 5ms/step - loss: 0.2612 - mae: 0.3922 - val_loss: 0.3036 - val_mae: 0.4160\n",
      "Epoch 15/120\n",
      "192/192 [==============================] - 1s 5ms/step - loss: 0.2324 - mae: 0.3760 - val_loss: 0.3465 - val_mae: 0.4478\n",
      "Epoch 16/120\n",
      "192/192 [==============================] - 1s 5ms/step - loss: 0.2188 - mae: 0.3652 - val_loss: 0.2691 - val_mae: 0.3974\n",
      "Epoch 17/120\n",
      "192/192 [==============================] - 1s 5ms/step - loss: 0.2015 - mae: 0.3490 - val_loss: 0.2806 - val_mae: 0.4038\n",
      "Epoch 18/120\n",
      "192/192 [==============================] - 1s 5ms/step - loss: 0.2165 - mae: 0.3644 - val_loss: 0.2361 - val_mae: 0.3727\n",
      "Epoch 19/120\n",
      "192/192 [==============================] - 1s 5ms/step - loss: 0.2073 - mae: 0.3584 - val_loss: 0.2781 - val_mae: 0.4085\n",
      "Epoch 20/120\n",
      "192/192 [==============================] - 1s 5ms/step - loss: 0.2165 - mae: 0.3680 - val_loss: 0.2649 - val_mae: 0.4024\n",
      "Epoch 21/120\n",
      "192/192 [==============================] - 1s 5ms/step - loss: 0.1981 - mae: 0.3515 - val_loss: 0.2308 - val_mae: 0.3710\n",
      "Epoch 22/120\n",
      "192/192 [==============================] - 1s 5ms/step - loss: 0.3249 - mae: 0.4415 - val_loss: 0.2133 - val_mae: 0.3518\n",
      "Epoch 23/120\n",
      "192/192 [==============================] - 1s 5ms/step - loss: 0.1989 - mae: 0.3554 - val_loss: 0.2093 - val_mae: 0.3503\n",
      "Epoch 24/120\n",
      "192/192 [==============================] - 1s 5ms/step - loss: 0.1797 - mae: 0.3373 - val_loss: 0.2022 - val_mae: 0.3454\n",
      "Epoch 25/120\n",
      "192/192 [==============================] - 1s 5ms/step - loss: 0.1687 - mae: 0.3277 - val_loss: 0.2060 - val_mae: 0.3552\n",
      "Epoch 26/120\n",
      "192/192 [==============================] - 1s 5ms/step - loss: 0.1686 - mae: 0.3227 - val_loss: 0.1931 - val_mae: 0.3421\n",
      "Epoch 27/120\n",
      "192/192 [==============================] - 1s 5ms/step - loss: 0.1898 - mae: 0.3455 - val_loss: 0.1973 - val_mae: 0.3465\n",
      "Epoch 28/120\n",
      "192/192 [==============================] - 1s 5ms/step - loss: 0.1654 - mae: 0.3220 - val_loss: 0.1894 - val_mae: 0.3363\n",
      "Epoch 29/120\n",
      "192/192 [==============================] - 1s 5ms/step - loss: 0.1714 - mae: 0.3318 - val_loss: 0.1925 - val_mae: 0.3408\n",
      "Epoch 30/120\n",
      "192/192 [==============================] - 1s 5ms/step - loss: 0.1782 - mae: 0.3308 - val_loss: 0.2372 - val_mae: 0.3683\n",
      "Epoch 31/120\n",
      "192/192 [==============================] - 1s 5ms/step - loss: 0.2046 - mae: 0.3603 - val_loss: 0.2017 - val_mae: 0.3477\n",
      "Epoch 32/120\n",
      "192/192 [==============================] - 1s 5ms/step - loss: 0.1838 - mae: 0.3436 - val_loss: 0.1864 - val_mae: 0.3371\n",
      "Epoch 33/120\n",
      "192/192 [==============================] - 1s 5ms/step - loss: 0.1706 - mae: 0.3297 - val_loss: 0.1943 - val_mae: 0.3412\n",
      "Epoch 34/120\n",
      "192/192 [==============================] - 1s 5ms/step - loss: 0.1735 - mae: 0.3303 - val_loss: 0.3060 - val_mae: 0.4524\n",
      "Epoch 35/120\n",
      "192/192 [==============================] - 1s 5ms/step - loss: 0.1919 - mae: 0.3456 - val_loss: 0.1871 - val_mae: 0.3403\n",
      "Epoch 36/120\n",
      "192/192 [==============================] - 1s 5ms/step - loss: 0.1667 - mae: 0.3284 - val_loss: 0.1778 - val_mae: 0.3321\n",
      "Epoch 37/120\n",
      "192/192 [==============================] - 1s 5ms/step - loss: 0.1759 - mae: 0.3318 - val_loss: 0.1784 - val_mae: 0.3291\n",
      "Epoch 38/120\n",
      "192/192 [==============================] - 1s 5ms/step - loss: 0.1806 - mae: 0.3381 - val_loss: 0.1777 - val_mae: 0.3335\n",
      "Epoch 39/120\n",
      "192/192 [==============================] - 1s 5ms/step - loss: 0.1548 - mae: 0.3141 - val_loss: 0.1841 - val_mae: 0.3302\n",
      "Epoch 40/120\n",
      "192/192 [==============================] - 1s 5ms/step - loss: 0.1748 - mae: 0.3315 - val_loss: 0.2472 - val_mae: 0.3948\n",
      "Epoch 41/120\n",
      "192/192 [==============================] - 1s 5ms/step - loss: 0.1941 - mae: 0.3494 - val_loss: 0.1872 - val_mae: 0.3399\n",
      "Epoch 42/120\n",
      "192/192 [==============================] - 1s 5ms/step - loss: 0.1779 - mae: 0.3351 - val_loss: 0.1762 - val_mae: 0.3265\n",
      "Epoch 43/120\n",
      "192/192 [==============================] - 1s 5ms/step - loss: 0.1752 - mae: 0.3321 - val_loss: 0.2035 - val_mae: 0.3549\n",
      "Epoch 44/120\n",
      "192/192 [==============================] - 1s 5ms/step - loss: 0.1733 - mae: 0.3376 - val_loss: 0.1859 - val_mae: 0.3362\n",
      "Epoch 45/120\n",
      "192/192 [==============================] - 1s 5ms/step - loss: 0.1716 - mae: 0.3290 - val_loss: 0.1770 - val_mae: 0.3291\n",
      "Epoch 46/120\n",
      "192/192 [==============================] - 1s 5ms/step - loss: 0.1620 - mae: 0.3259 - val_loss: 0.2158 - val_mae: 0.3684\n",
      "Epoch 47/120\n",
      "192/192 [==============================] - 1s 5ms/step - loss: 0.1670 - mae: 0.3262 - val_loss: 0.2125 - val_mae: 0.3663\n",
      "Epoch 48/120\n",
      "192/192 [==============================] - 1s 5ms/step - loss: 0.1741 - mae: 0.3365 - val_loss: 0.1796 - val_mae: 0.3311\n",
      "Epoch 49/120\n",
      "192/192 [==============================] - 1s 5ms/step - loss: 0.1636 - mae: 0.3210 - val_loss: 0.1828 - val_mae: 0.3376\n",
      "Epoch 50/120\n",
      "192/192 [==============================] - 1s 5ms/step - loss: 0.1676 - mae: 0.3232 - val_loss: 0.1708 - val_mae: 0.3236\n",
      "Epoch 51/120\n",
      "192/192 [==============================] - 1s 5ms/step - loss: 0.1679 - mae: 0.3259 - val_loss: 0.1698 - val_mae: 0.3245\n",
      "Epoch 52/120\n",
      "192/192 [==============================] - 1s 5ms/step - loss: 0.1641 - mae: 0.3244 - val_loss: 0.1729 - val_mae: 0.3265\n",
      "Epoch 53/120\n",
      "192/192 [==============================] - 1s 5ms/step - loss: 0.1625 - mae: 0.3174 - val_loss: 0.1887 - val_mae: 0.3403\n",
      "Epoch 54/120\n",
      "192/192 [==============================] - 1s 5ms/step - loss: 0.1776 - mae: 0.3378 - val_loss: 0.1686 - val_mae: 0.3216\n",
      "Epoch 55/120\n",
      "192/192 [==============================] - 1s 5ms/step - loss: 0.1716 - mae: 0.3281 - val_loss: 0.1765 - val_mae: 0.3282\n",
      "Epoch 56/120\n",
      "192/192 [==============================] - 1s 5ms/step - loss: 0.1709 - mae: 0.3271 - val_loss: 0.1786 - val_mae: 0.3324\n",
      "Epoch 57/120\n",
      "192/192 [==============================] - 1s 5ms/step - loss: 0.1647 - mae: 0.3242 - val_loss: 0.1720 - val_mae: 0.3228\n",
      "Epoch 58/120\n",
      "192/192 [==============================] - 1s 5ms/step - loss: 0.1687 - mae: 0.3303 - val_loss: 0.1823 - val_mae: 0.3378\n",
      "Epoch 59/120\n",
      "192/192 [==============================] - 1s 5ms/step - loss: 0.1652 - mae: 0.3254 - val_loss: 0.1801 - val_mae: 0.3325\n",
      "Epoch 60/120\n",
      "192/192 [==============================] - 1s 5ms/step - loss: 0.1802 - mae: 0.3399 - val_loss: 0.1767 - val_mae: 0.3238\n",
      "Epoch 61/120\n",
      "192/192 [==============================] - 1s 5ms/step - loss: 0.1691 - mae: 0.3249 - val_loss: 0.1848 - val_mae: 0.3357\n",
      "Epoch 62/120\n",
      "192/192 [==============================] - 1s 5ms/step - loss: 0.1570 - mae: 0.3163 - val_loss: 0.1683 - val_mae: 0.3236\n",
      "Epoch 63/120\n",
      "192/192 [==============================] - 1s 5ms/step - loss: 0.1898 - mae: 0.3478 - val_loss: 0.1773 - val_mae: 0.3313\n",
      "Epoch 64/120\n",
      "192/192 [==============================] - 1s 5ms/step - loss: 0.1724 - mae: 0.3308 - val_loss: 0.1714 - val_mae: 0.3245\n",
      "Epoch 65/120\n",
      "192/192 [==============================] - 1s 5ms/step - loss: 0.1567 - mae: 0.3156 - val_loss: 0.1672 - val_mae: 0.3246\n",
      "Epoch 66/120\n",
      "192/192 [==============================] - 1s 5ms/step - loss: 0.1627 - mae: 0.3246 - val_loss: 0.1668 - val_mae: 0.3214\n",
      "Epoch 67/120\n",
      "192/192 [==============================] - 1s 5ms/step - loss: 0.1690 - mae: 0.3284 - val_loss: 0.1649 - val_mae: 0.3211\n",
      "Epoch 68/120\n",
      "192/192 [==============================] - 1s 5ms/step - loss: 0.1515 - mae: 0.3101 - val_loss: 0.1723 - val_mae: 0.3224\n",
      "Epoch 69/120\n",
      "192/192 [==============================] - 1s 5ms/step - loss: 0.1559 - mae: 0.3177 - val_loss: 0.1751 - val_mae: 0.3266\n",
      "Epoch 70/120\n",
      "192/192 [==============================] - 1s 5ms/step - loss: 0.1560 - mae: 0.3188 - val_loss: 0.1662 - val_mae: 0.3241\n",
      "Epoch 71/120\n",
      "192/192 [==============================] - 1s 5ms/step - loss: 0.1547 - mae: 0.3138 - val_loss: 0.1942 - val_mae: 0.3491\n",
      "Epoch 72/120\n",
      "192/192 [==============================] - 1s 5ms/step - loss: 0.1698 - mae: 0.3298 - val_loss: 0.1853 - val_mae: 0.3386\n",
      "Epoch 73/120\n",
      "192/192 [==============================] - 1s 5ms/step - loss: 0.1593 - mae: 0.3154 - val_loss: 0.1693 - val_mae: 0.3220\n",
      "Epoch 74/120\n",
      "192/192 [==============================] - 1s 5ms/step - loss: 0.1588 - mae: 0.3203 - val_loss: 0.1711 - val_mae: 0.3265\n",
      "Epoch 75/120\n",
      "192/192 [==============================] - 1s 5ms/step - loss: 0.1462 - mae: 0.3054 - val_loss: 0.1746 - val_mae: 0.3284\n",
      "Epoch 76/120\n",
      "192/192 [==============================] - 1s 5ms/step - loss: 0.1517 - mae: 0.3111 - val_loss: 0.1728 - val_mae: 0.3246\n",
      "Epoch 77/120\n",
      "192/192 [==============================] - 1s 5ms/step - loss: 0.1602 - mae: 0.3206 - val_loss: 0.1845 - val_mae: 0.3384\n"
     ]
    }
   ],
   "source": [
    "best_model = BuildModel(model_name='best_temp_smooth_model.h5', length=30, layers_num=1,\\\n",
    "                        layers_type='LSTM', units=80, dropout=0, g_filt=1, epochs=120, batch_size=10,\\\n",
    "                        patience=10)\n",
    "\n",
    "best_model.setupData(temp_train)\n",
    "best_model.fitModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W1XG7tIUBk7Q"
   },
   "outputs": [],
   "source": [
    "#load best performer\n",
    "best_model.loadModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 388
    },
    "executionInfo": {
     "elapsed": 1129,
     "status": "ok",
     "timestamp": 1621888714080,
     "user": {
      "displayName": "1990JAF",
      "photoUrl": "",
      "userId": "13601147066600634770"
     },
     "user_tz": -60
    },
    "id": "cPLhXNO3Bkzs",
    "outputId": "e2b0734b-2e9d-4a2b-eea8-1dd567ff852e"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#predict a week\n",
    "week_pred = best_model.predAhead(7)\n",
    "\n",
    "#plot against test week\n",
    "best_model.plotPreds(week_pred, temp_test, ylabel='temp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GOCqzYdxBm9P"
   },
   "source": [
    "Now that is much better compared to the previous notebook!\n",
    "\n",
    "The first value is pretty close to its target and the second is even right on it! Given that the model is only supposed to predict one step into the future, I am quite pleased with this.\n",
    "\n",
    "The other predictions are built upon the predictions that came before them and so we can expect greater error as we predict further into the future.\n",
    "\n",
    "This model can now be deployed with the API feeding it live data.\n",
    "\n",
    "The other weather variables are modelled in yet more notebooks that are going to be rather similar to this one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0THN938BHhJT"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyO70RqG4eTbzVb8Pddtv5Jy",
   "collapsed_sections": [],
   "mount_file_id": "17IG77FL_jzzulw5E-pB8pOQNzlEqEXHa",
   "name": "model_build_smooth.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
